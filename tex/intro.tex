\chapter{Introduction}

There are many problems in computer science that, from the standpoint of finding a solution faster, appear amenable to the use of concurrency in their solution.  There are many more where opportunities for concurrency are less apparent and where applying concurrent processing is much less straightforward, but which nevertheless can benefit from it.  For example, many problems that arise in image processing, or which can be modelled graphically, fall into one or the other of those categories.  Modelling these problems with a computational model that embraces concurrency at its core can prove beneficial, both in terms of exposing new ways to structure computations on existing computer implementations, \eg{} \cite{GimelFarb2013a,Nicolescu2014b}, and (potentially) in terms of providing blueprints for the efficient use of new hardware implementations in the future.\footnote{One need only consider the example and future importance of such computing luminaries as Babbage, Lovelace and Boole, all working before a complete functioning electronic or mechanical computer was created, to see the truth of this.}

This dissertation takes precisely that approach.  Pre-existing problems drawn from the history of computer science are modelled in an inherently concurrent theory of computation, \gls{cps}, seeking the lowest possible theoretical time complexity for each.  Those highly concurrent models are then translated into operative computer programs on current widely available computer hardware.  The theoretical computational model used in this dissertation is \gls{cps} \cite{Nicolescu2018}, one of the branches of the broader field of \gls{mc} (also known as \gls{ps}) \cite{Paun2010b,Paun2002}.  Different hardware, programming languages and techniques have been used for the electronic computer implementations in the course of this work, varying based on availability, apparent fit with the relevant \gls{cps} solution, and lessons learned from earlier development.

\Gls{mc} was chosen over other computational models with an emphasis on concurrency because it appears to be a highly promising developing area, especially \gls{cps}.  While \gls{mc} is bio-inspired \cite{Paun2000}, its utility extends far beyond simulating biological systems.  In the original concept (still in use), each \gls{compartment} delimited by a membrane is considered a separate processor with unbounded capacity.  Furthermore, the membranes are frequently considered to have the capability of dividing and creating new membranes and thus processors.  This ability for membrane creation, combined with unbounded space capacity, has long been known to enable polynomial-time solutions to problems that have traditionally been considered intractable, by trading space for time \cite{Paun1999a,Sosik2003}.
Other particularly notable calculi are briefly summarised, but the focus of this work is very much on \gls{ps}, and specifically \gls{cps}.

All forms of \gls{ps} operate based on sets of rules associated with delimited \glspl{compartment}, which typically contain multisets of objects. The main advantage of \gls{cps} over other \gls{ps} is its powerful concept of unification (see more in \vref{sec:cps:unification}), which permits the rules to adapt themselves automatically at ``runtime'' to the contents of the relevant \gls{compartment}(s).  By contrast, other forms of \gls{ps} typically require some degree of pre-processing to adapt uniform or semi-uniform families of rules to the given circumstances.  This tends to make interpretation of the rules more difficult, but also can mean that the solution to the problem at hand sometimes is partially pre-computed before the \glspl{ps} ever starts working.

The chief aim of this work is to describe new solutions to pre-existing problems, where the new solutions provide superior time complexity characteristics to other known solutions.  That is, applying relatively new and lesser-known tools of computer science to known problems, with the goal of finding novel solutions with a low time complexity, where low is considered linear complexity -- \bigoh{n} -- or better.  
This is achieved by describing these solutions in \gls{cps}, which is high-level and declarative, and should make each problem's overarching operation and core elements evident.
A secondary aim of this work is to try to find novel, useful ways to implement the proposed solutions to the pre-existing problems on current computers, even when the \gls{cps} model does not map cleanly to available hardware.  

\section{Outline and Key Results}

This dissertation proceeds as follows: \Cref{chap:back} summarises some alternative models for concurrent computation -- none of which have quite the same properties as \gls{ps} -- that have been explored in the past;  explains the concept of synchronous vs asynchronous communication in distributed systems (relevant later on);  provides an overview of a programming approach called \gls{cml} \cite{Reppy2007,Reppy1991}, which appears to be a good fit to some of the presented \gls{cps} \glspl{ruleset}; and finally surveys the state of \gls{mc} as at the time of writing.  Next, \cref{chap:cpsystems} fairly comprehensively documents \gls{cps} as used recently.  The various elements of \gls{cps} that have been defined and are still in use -- many of which are used later in this work too -- are shown, before demonstrating how well-known data structures may be modelled in \gls{cps}.  Combined with the examples provided in the subsequent chapters, this should permit the interested reader to start modelling problems in \gls{cps} themselves.

% \Crefrange{chap:tsp}{chap:median} each investigate one or more established problems exhibiting significant potential for concurrency.  Specifically: \cref{chap:tsp} focuses on the \gls{hpp}, \gls{hcp} and \gls{tsp} \cite{Applegate2006,Cook2012}; \cref{chap:gcol} on the \gls{gcp} \cite{Lewis2016}; and, \cref{chap:median} on the \gls{medianfilter}, a \gls{mwt} operation performed in image processing \cite{Fisher2016,Gimelfarb2018}.
\Crefrange{chap:tsp}{chap:median} each investigate one or more established problems exhibiting significant potential for concurrency.  Specifically:  \cref{chap:tsp} focuses on the \gls{hpp}, \gls{hcp} and \gls{tsp} \cite{Applegate2006,Cook2012}; \cref{chap:gcol} on the \gls{gcp} \cite{Lewis2016}; and, \cref{chap:median} on the \gls{medianfilter}, a common operation in image processing \cite{Fisher2016,Gimelfarb2018}.

\Cref{chap:nmp} explores the idea of \gls{nmp}.  Motivated initially by \gls{lbp} \cite{Sun2003,Felzenszwalb2006,Felzenszwalb2011} as used in \gls{sm} \cite{Sinha2020,Tippetts2016,Scharstein2002}, this \namecref{chap:nmp} investigates the situation where individual logical \glspl{pe} are arranged in a lattice and exchange messages as part of computing the solution to a given problem, but every \gls{pe} can only send a message to each connected neighbouring \gls{pe} \emph{after} it has received messages from every other one of its neighbours.  There are two main phases of the core computation process for \gls{nmp}:
\begin{inparaenum}[a)]
\item inter-\gls{pe} communications between neighbours via message passing; and
\item intra-\gls{pe} computations to update internal data and produce new messages.
\end{inparaenum}  The focus in \cref{chap:nmp} is on the first of these only, with the second abstracted over for current purposes using an oracle with which \glspl{pe} communicate to perform the required updates.

All of \crefrange{chap:tsp}{chap:nmp} provide one or more novel \gls{cps} \glspl{ruleset} with an accompanying explanation, at least one worked example of said \gls{ruleset}, analysis of the proposed \gls{cps} solution including (where applicable) comparisons to related solutions found elsewhere in \gls{ps}, and the results of at least one experimental computer simulation.

The \gls{cps} solutions to the \gls{hpp}, \gls{hcp} and \gls{tsp} in \cref{chap:tsp} all have a time complexity that is linear with respect to the number of nodes in the graph -- \ie{} it is \bigoh{n}, where \(n\) is the order of the graph -- and requires a total of four (for the \gls{hpp}) or five (for the \gls{hcp} and \gls{tsp}) fixed rules for \emph{every} graph with a possible solution.  Simulations are implemented in \fsharp{}, Erlang and Prolog, with the Prolog simulation in particular closely matching the \glspl{cps}.

\Cref{chap:gcol} experiments with using \gls{cml} to implement a pre-existing \gls{ps} solution to the \gls{gcp}, finding that while the implementation is effective, there is less communication involved than initially expected.  Then, it describes a \gls{cps} solution to the \gls{gcp} which has a time complexity of \bigoh{n}, where \(n\) is the order of the graph, for graphs where a \(k\)-colouring is possible.  For graphs where no such colouring is possible, it has a time complexity of \bigoh{k}.  \Cref{chap:gcol} represents the first known application of \gls{cml} to any type of \gls{ps} (followed by \cref{chap:median} as the second).

\Gls{medianfilter}ing is structurally more challenging than many similar \gls{mwt} problems in image processing.  It implies a considerable amount of numerical sorting, and the necessity of preserving the image data prevents the use of typical enhancements seen with other algorithms, which generally perform a simpler reduction over the data.  The \glspl{cps} presented in \cref{chap:median} demonstrates conclusively that sufficiently large-scale concurrency can solve the \gls{medianfilter} problem in constant time, however.  The experimental results suggest that current CPUs are far from managing this level of concurrency, though, with the \gls{cml} implementation proving to be by far the slowest --- at minimum 20 times slower than the fastest alternative.

\Cref{chap:nmp} analyses the traditional \gls{gs} approach to \gls{nmp}, where every \gls{pe} waits for all other \glspl{pe} to finish exchanging messages before progressing further itself; and, an asynchronous alternative where each \gls{pe} sends the succeeding message to each neighbour as soon as it receives the necessary input messages.  From these, an intermediate \gls{ls} approach, where each \gls{pe} waits only for all of its own expected messages to arrive before progressing, arises naturally and is also explored.

The behaviour and comparative running times required for each variant are investigated empirically, and the \gls{ls} method found strictly superior to the \gls{gs} method -- exhibiting identical messaging behaviour but typically running 5-13\% faster -- while the asynchronous method is, in general, roughly 10\% faster again than the \gls{ls} yet computes final results that ordinarily are within roughly 0.1\% of those of the \gls{ls} method.

Finally, \cref{chap:conc} concludes this dissertation by summarising the work described here and suggesting multiple possible avenues for future research.