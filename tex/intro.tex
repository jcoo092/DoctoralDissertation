\chapter{Introduction}

\pagenumbering{arabic}

\begin{anfxwarning}{What goes in the intro?}
From \url{https://abacus.bates.edu/~ganderso/biology/resources/writing/HTWsections.html}:  ``the Introduction must answer the questions, "What was I studying? Why was it an important question? What did we know about it before I did this study? How will this study advance our knowledge?"''

See also \url{https://www.thephdproofreaders.com/writing/how-to-write-a-thesis-introduction/} and \url{https://papersowl.com/blog/phd-thesis-introduction}
\end{anfxwarning}

\begin{anfxnote}{More tips}
From \url{https://student.unsw.edu.au/introductions}:

What types of information should you include in your introduction? 

In the introduction of your thesis, you’ll be trying to do three main things, which are called Moves:

\begin{itemize}
    \item Move 1 establish your territory (say what the topic is about)
    \item Move 2 establish a niche (show why there needs to be further research on your topic)
    \item Move 3 introduce the current research (make hypotheses; state the research questions)
\end{itemize}

Each Move has a number of stages. Depending on what you need to say in your introduction, you might use one or more stages. Table 1 provides you with a list of the most commonly occurring stages of introductions in Honours theses (colour-coded to show the Moves). You will also find examples of Introductions, divided into stages with sample sentence extracts. Once you’ve looked at Examples 1 and 2, try the exercise that follows.

Most thesis introductions include SOME (but not all) of the stages listed below. There are variations between different Schools and between different theses, depending on the purpose of the thesis.

Stages in a thesis introduction
\begin{enumerate}
    \item state the general topic and give some background
    \item provide a review of the literature related to the topic
    \item define the terms and scope of the topic
    \item outline the current situation
    \item evaluate the current situation (advantages/ disadvantages) and identify the gap
    \item identify the importance of the proposed research
    \item state the research problem/ questions
    \item state the research aims and/or research objectives
    \item state the hypotheses
    \item outline the order of information in the thesis
    \item outline the methodology
\end{enumerate}

1-3 are move 1, 4-5 are move 2 and 6-11 are move 3.

\end{anfxnote}

% \section{Research Questions}
% \begin{enumerate}
%     \item Does modelling \gls{nmp} in a theoretical setting like \gls{cps} help in some way?
%     \item Does an asynchronous model of \gls{nmp} reveal any benefit over the traditional synchronous versions?
%     \item Does the practical implementation of \gls{nmp}, applied to \gls{bp} for \gls{sm} show any improvement over other techniques?
% \end{enumerate}

% \subsection{Research Question One}

% \subsection{Research Question Two}

% \subsection{Research Question Three}

% \section{Hypotheses}

% \subsection{Hypothesis One}

% \subsection{Hypothesis Two}

% \subsection{Hypothesis Three}

% \section{Brief background}

There are many problems in computer science which appear amenable to the use of concurrency in their solution, from the standpoint of finding a solution quicker.  There are many more where opportunities for concurrency are less obvious, and where applying concurrent processing is much less straightforward, but which nevertheless can benefit from it.  For example, many problems that arise in image processing, or which can be modelled graphically, fall into one or the other of those categories.  Modelling these problems with a computational model that embraces concurrency at its core can prove beneficial, both in terms of exposing new ways to structure computations on existing computer implementations, \eg{} \cite{GimelFarb2013a,Nicolescu2014b}, and (potentially) in terms of providing blueprints for the efficient use of new hardware implementations in the future.\footnote{One needs only consider the example and future importance of such computing luminaries as Babbage, Lovelace and Boole, all working before a complete working electronic or mechanical computer was created, to see the truth of this.}

This dissertation takes exactly that approach.  Pre-existing problems drawn from the history of computer science are modelled in an inherently concurrent theory of computation, seeking the lowest possible theoretical time complexity for each.  Those highly concurrent models are then translated into operative computer programs on current widely available computer hardware.  The theoretical computational model used in this dissertation is \gls{cps} \cite{Nicolescu2018}, one of the branches of the broader field of \gls{mc} (also known as \gls{ps}) \cite{Paun2010b,Paun2002}.  Different hardware, programming languages and techniques have been used in the course of this work, varying based on availability, apparent fit with the relevant \gls{cps} solution, and lessons learned from earlier development.

\Gls{mc} was chosen over other computational models with an emphasis on concurrency because it appears to be a highly promising developing area, especially \gls{cps}.  While it is bio-inspired \cite{Paun2000}, the utility of \gls{mc} extends far beyond simulating biological systems.  In the original concept (which is still in use), each compartment delimited by a membrane is considered to be a separate processor with unbounded capacity.  Furthermore, the membranes are frequently considered to have the capability of dividing and creating new membranes, and thus processors.  This ability combined with the unbounded capacity have long been known to enable the polynomial time solution to problems that have traditionally been considered more difficult \cite{Paun1999a,Sosik2003}.
Other particularly notable calculi are briefly summarised, but the focus of this work is very much on \gls{ps}, and specifically \gls{cps}.%, both to acknowledge them and to show that \gls{mc} is not the only one which can be used if one wishes.

All forms of \gls{ps} operate on the basis of sets of rules associated to delimited compartments, which typically contain multisets of objects. The main advantage of \gls{cps} over other \gls{ps} is its powerful concept of unification (see more in \vref{sec:cps:unification}), which permits the rules to adapt themselves automatically at `runtime' to the contents of the relevant compartment(s).  By contrast, other forms of \gls{ps} typically require some degree of pre-processing to adapt uniform or semi-uniform families of \glspl{ruleset} to the given circumstances.  This tends to make interpretation of the rules much more difficult, but also can mean that the solution to the problem at hand sometimes is partially pre-computed before the \glspl{ps} ever starts.

The chief aims of this work are threefold.  Firstly, to describe new solutions to pre-existing problems which provide superior time complexity characteristics to other known solutions.  That is, applying relatively new and lesser-known tools of computer science to problems with known solutions, with the goal of finding novel solutions with a lower time complexity.  Secondly, to try to find novel, useful, ways to implement solutions to the pre-existing problems on current hardware, even when the \gls{cps} model does not clearly map directly to the hardware.  Thirdly, describing these solutions in \gls{cps}, which is high-level and declarative, should make the overarching operation and core elements of each problem evident.  Of course, these goals are symbiotic, not mutually exclusive.

\section{Outline}

This dissertation proceeds as follows: \Cref{chap:lr} summarises some alternative models for concurrent computation -- none of which have quite the same properties as \gls{ps} -- that have been explored in the past; provides an overview of a programming approach called \gls{cml} \cite{Reppy2007,Reppy1991}, which appears to be a good fit in many ways to some of the \gls{cps} solutions and of which heavy use is made in \cref{chap:gcol,chap:median}; and finally surveys the state of \gls{mc} as at the time of writing.  Next, \cref{chap:cpsystems} fairly comprehensively documents \gls{cps} as used in recent publications.  The various elements of \gls{cps} that have been defined and are still in use -- many of which are used later in this work too -- are shown, before demonstrating how traditional computer science data structures may be modelled in \gls{cps}.  Combined with the examples provided in the subsequent chapters, this should permit the interested reader to start modelling problems in \gls{cps} themselves.
% \Cref{chap:cpsystems} broadly summarises of \gls{cps} as it stands at the time of submission.%  Finally, methods to perform a variety of statistical operations over numeric data are expounded.

\Crefrange{chap:tsp}{chap:median} each investigate one or more problems exhibiting significant potential for concurrency.  Specifically: \cref{chap:tsp} focuses on the \gls{hpp}, \gls{hcp} and \gls{tsp} \cite{Applegate2006,Cook2012}; \cref{chap:gcol} on the \gls{gcp} \cite{Lewis2016}; and, \cref{chap:median} on the \gls{medianfilter}, a \gls{mwt} operation performed in image processing \cite{Fisher2016,Gimelfarb2018}.  In each of these chapters, the problem has first been modelled using \gls{cps}, with one or more \glspl{ruleset} developed and analysed, providing a highly concurrent solution to the problem.  Then, the \gls{cps} approach has been investigated empirically, using one or more computer programs.  These programs validated the correctness of the \gls{cps} \glspl{ruleset}.  They also sometimes highlight interesting aspects of either the problem or the proposed \gls{cps} solution.

Finally, \cref{chap:nmp} explores the idea of \gls{nmp}.  %on \gls{nmp}, \fxerror{Finish describing NMP}{a problem where...}.
Originally motivated by \gls{lbp} \cite{Sun2003,Felzenszwalb2006,Felzenszwalb2011} as used in \gls{sm} \cite{Sinha2020,Tippetts2016,Scharstein2002}, this \namecref{chap:nmp} investigates the situation where individual logical \glspl{pe} are arranged in a lattice and exchange messages as part of computing the solution to a given problem, but every \gls{pe} can only send a message to each connected neighbouring \gls{pe} \emph{after} it has received messages from every other one of its neighbours.  There are two main phases of the core computation process for \gls{nmp}:
\begin{inparaenum}[a)]
\item inter-\gls{pe} communications between neighbours via message passing; and
\item intra-\gls{pe} computations to update internal data and produce new messages.
\end{inparaenum}  The focus in \cref{chap:nmp} is on the first of these only, with the second abstracted over for current purposes using an oracle with which \glspl{pe} communicate to perform the required updates.

Every chapter provides one or more novel \gls{cps} \glspl{ruleset} with an accompanying explanation, at least one worked example of said \gls{ruleset}, some analysis of the proposed \gls{cps} solution including comparisons to related solutions found elsewhere in \gls{ps}, and the results of at least one experimental computer simulation.

% \begin{anfxnote}
% Where I summarise the upcoming dissertation (this section probably needs a different/better name).  Where in the intro do I explain the novelty?
% \end{anfxnote}

% Background information on \gls{mc} and \gls{cml}, as well as other theoretical models for concurrent computation, is provided in \cref{chap:lr}.

% \section{Highlight contributions \& key results}

\section{Key Results}

The \gls{cps} solutions to the \gls{hpp}, \gls{hcp} and \gls{tsp} in \cref{chap:tsp} all have a time complexity that is linear with respect to the number of nodes in the graph -- \ie{} it is \bigoh{n}, where \(n\) is the order of the graph -- and requires a total of four (for the \gls{hpp}) or five (for the \gls{hcp} and \gls{tsp}) fixed rules for \emph{every} graph with a possible solution.  Simulations are implemented in \fsharp{}, Erlang and Prolog, with the Prolog simulation in particular closely matching the \glspl{cps}.

\Cref{chap:gcol} experiments with using \gls{cml} to implement a pre-existing \gls{ps} solution to the \gls{gcp}, finding that while the implementation is effective, there is less communication involved than initially expected.  Then, it describes a \gls{cps} solution to the \gls{gcp} which has a time complexity of \bigoh{n}, where \(n\) is the number of nodes in the graph, for graphs where a \(k\)-colouring is possible.  For graphs where no such colouring is possible, it has a time complexity of \bigoh{k}.  \Cref{chap:gcol} represents the first known application of \gls{cml} to any type of \gls{ps} (followed by \cref{chap:median} as the second).

\Gls{medianfilter}ing is structurally more challenging than many similar \gls{mwt} problems in image processing.  It implies a considerable amount of numerical sorting, and the necessity of preserving the image data prevents the use of typical enhancements to other algorithms, which generally perform a simpler piecewise reduction over the data.  The \glspl{cps} presented in \cref{chap:median} demonstrates conclusively that sufficiently large-scale concurrency can solve the \gls{medianfilter} problem in constant time, however.  The experimental results suggest that current CPUs are far from managing this level of concurrency, though, with the \gls{cml} implementation proving to be by far the slowest --- at minimum 20 times slower than the fastest alternative.

\Cref{chap:nmp} analyses the traditional \gls{gs} approach to \gls{nmp}, where every \gls{pe} waits until every other \gls{pe} has finished exchanging messages before progressing further itself, and an asynchronous alternative where each \gls{pe} sends the next message to each neighbour as soon as it has received the necessary input messages.  From these, an intermediate \gls{ls} approach, where each \gls{pe} waits only for all of its own expected messages to arrive before progressing, naturally arises and is explored also.

The behaviour and comparative running times required for each variant are investigated empirically, and it is found that the \gls{ls} method is strictly superior to the \gls{gs} method -- exhibiting the same messaging behaviour but typically running 5-13\% faster -- while the asynchronous method is, in general, roughly 10\% faster again than the \gls{ls} yet computes final results that ordinarily are within roughly 0.1\% of those of the \gls{ls} method.

Finally, \cref{chap:conc} concludes this dissertation by summarising the work described here, and suggesting multiple possible avenues for future research.