\chapter{\label{chap:conc}Conclusion}

% \begin{anfxerror}{Backwards conclusion}
% Jos√© suggested an approach where you go backwards in the conclusion, starting with the most recent section and eventually reaching the start again.  This, presumably, is for instances where each earlier bit contributes to one or more later bits, and so you say \enquote{we did x, using y that we defined earlier} sort of thing.
% \end{anfxerror}

As suggested by the title, this dissertation has focused on highly concurrent solutions to computing problems.  These are, broadly, considered challenging problems (in the exact case at least) for modern electronic computers, but the high levels of concurrency -- combined with taking advantage of the unbounded memory space of \gls{mc} -- can significantly reduce their time complexity.  Each solution requires either constant time or linear time dependent on a core aspect of the problem.

The specific problems addressed were:
\begin{inparaenum}[(1)]
\item The \gls{hpp}, \gls{hcp} and \gls{tsp};
\item The \gls{gcp};
\item Image \gls{medianfilter}ing; and,
\item \gls{nmp}.
\end{inparaenum}
The first three are well-known problems with histories stretching back decades.  The last, \gls{nmp}, is (to the best of the author's knowledge) newly abstracted out from \gls{lbp}, initially motivated by attempts to model a pre-existing algorithm to perform \gls{lbp} for \gls{sm} in \gls{cps}.

None of the \glspl{ruleset} require customisation or adjustment for a given problem.  Instead, the \gls{cps} rules adapt automatically, so long as the problem to be solved is adequately specified.  This is unlike solutions sometimes seen in other forms of \gls{ps}, where a uniform or semi-uniform family of rules is defined, requiring some (potentially significant) level of pre-processing before the \glspl{ps} is applied to find the solution.  For the first three problems, the same \gls{ruleset} can be used across every \gls{tlc}.  Even in the case of \gls{fne} \gls{nmp}, all that is required is assigning a reduced \gls{ruleset} derived from the base one to the \gls{pe} with fewer than four neighbours, \ie{} those on the edge, or in the corner, of the grid.

The various systems presented were investigated experimentally, using a variety of approaches.  Direct translations of the \gls{tsp} algorithm were implemented in three different programming languages, namely \fsharp{}, Erlang and Prolog.  All three worked well for small graphs, but quickly grew to consume more memory than the test system had available.  The Prolog implementation was notable, however, for being an extremely close match to the specified \gls{cps} \gls{ruleset}, requiring only 17 lines (including whitespace) to specify the complete system.

The algorithm to solve the \gls{gcp} was translated into a communicative system using an implementation of \gls{cml}, \hopac{} in \fsharp{}.  While the implementation worked well, it also highlighted that there was, in fact, relatively little communication involved, and the system had more in common with the \gls{tsp} implementation than first presumed.  Comparing the results from the simulation on various graphs against using an earlier \gls{skps} solution using \gls{mecosim} suggested that -- as largely expected due to the nature of the \adhoc{} vs general simulations (see \vref{sec:back:simulators}) -- the \gls{cps} simulation performed better.  Doing so also revealed a curious behaviour of \gls{mecosim}, where two almost-identical graphs produced markedly different runtime characteristics.

\Glspl{ruleset} to perform a variety of statistical operations were detailed in \cref{chap:median}.  All of these \glspl{ruleset} have a constant time complexity, \ie{} they are \bigoh{1}.  They also can be used as primitives and composed together to create further operations, as demonstrated in \eg{} \cref{sec:median:mean,sec:median:mode,sec:median:selection}.  The practical application and combination of these rules was then demonstrated by defining a \gls{ruleset} to perform \gls{medianfilter}ing on a grid representing an image.  Different approaches to implementing the \gls{medianfilter} were experimented with, including one based on communicative pixels -- as with the \gls{cps} solution -- and created using \gls{cml}.  In this instance, the \gls{cml} implementation performed much worse overall than the other two presented, both in terms of running time on a conventional CPU and clarity of code.

While the development of \gls{nmp} in \cref{chap:nmp} was initially based on attempting to model \gls{lbp} in \gls{cps}, the \gls{nmp} variants presented generalise beyond \gls{lbp}.  In particular, \cref{chap:nmp} presented a generic framework for the messaging aspect, but did not prescribe the internal computations each \gls{pe} should perform.  Instead, an oracle stood in for that part of the process, with the intention that the oracle is replaced as appropriate for a given problem.  The distinguishing characteristic of \gls{nmp} compared to other messaging schemes is that each message a \gls{pe} sends to a given neighbour depends upon the messages received from the other neighbours.  The initial focus of the work was on deriving the asynchronous variant from the original \gls{gs} approach, and the \gls{ls} arose naturally out of this.

The asynchronous variant in particular was analysed and its behaviour explored in \cref{sec:nmp:analysis}.  It was demonstrated that the asynchronous variant sends the same number of messages as the \gls{gs} variant, using information that is at least as recent, giving rise to \emph{generational confluence}.  The spread of `influence' from a centre \gls{pe} to others on a grid was also visualised under differing communications channel conditions and using different formulae to update the messages.

Empirical testing of a simulation of the asynchronous variant in a \gls{fne} grid configuration confirmed that it had the expected messaging behaviour.  Moreover, detailed simulations of each variant showed that the \gls{ls} variant (very nearly) always outperforms the \gls{gs} variant -- typically by a factor of approximately 5-13\%, with the difference typically becoming larger as the size of the lattice and number of processors available both increase -- with no change in the final computed results, while the asynchronous variant is consistently faster again, often by a factor of approximately 10\% compared to the \gls{ls} variant.  The final results computed by it also tend to be near-identical to the other two variants, often within roughly 0.1\%.

\section{Contributions}
This work makes the following main contributions:

\begin{itemize}
    \item \Cref{chap:cpsystems} provides an extensive overview of \gls{cps} as it stands at the time of writing.  This is believed to be the most comprehensive summary to date.
    \item \Cref{chap:tsp} presents solutions to the \gls{hpp} and \gls{hcp}, which require a total of \(n + 1\) or \(n + 3\) steps respectively, where \(n\) is the number of nodes in the graph, with the former needing three \gls{cps} rules and the latter four.  Furthermore, it also builds on the \gls{hcp} solution to solve the \gls{tsp}, requiring the same number of steps and one more rule than the \gls{hcp}'s solution.  No earlier linear time \gls{mc} solution to the \gls{tsp} is known.
    \item \Cref{chap:gcol} presents a solution to the \gls{gcp}, which finds a valid colouring (or declares no valid colouring is possible) using only six rules, with a worst-case time complexity linear to the size of the graph.  Importantly, it works with \emph{any} number of colours, instead of the usual fixed number studied by most of the literature.
    \item \Glspl{ruleset} for many standard statistical operations on numerical multisets in \gls{cps} were provided in \cref{chap:median}.  These included:
    \begin{inparaenum}[(i)]
        \item finding the minimum and maximum;
        \item counting the number of elements, and counting the frequency of each distinct element;
        \item finding the sum, mean, and mode;
        \item sorting; and,
        \item selecting the \(n^{\text{th}}\) element from a set.
    \end{inparaenum}
    All of these, besides the minimum and maximum, are not known to have been presented earlier.  Applying these rules to the \gls{medianfilter} is also thought to be the first time this particular image operation has been modelled in any form of \gls{ps}.
    \item Building on \cref{chap:median}, and taking heavy inspiration from \gls{lbp}, \cref{chap:nmp} investigated message passing between individual \glspl{pe} on a lattice. Importantly, in the scenario under study, each message sent out depends on the last messages received from every neighbour of the preparing \gls{pe} \emph{apart} from the intended recipient of the current message.  This scenario was termed here ``\glsxtrlong{nmp}''.  \Glspl{ruleset} for \gls{gs}, \gls{ls} and asynchronous messaging approaches were provided.  To the best of the author's knowledge, this is the first time this type of message passing has been modelled directly according to the conceptual \gls{nmp} principles of message passing between \glspl{pe}, as opposed to other more simplistic approaches.  Importantly, this approach is also more general, as it can adapt to any form of connectivity on the lattice, beyond a grid, simply by adjusting the connections between \glspl{pe} appropriately.
    \item Empirical results assessing the systems described above, implemented to run on relatively common modern computer hardware, are provided.  These results have explored various approaches to translating \gls{cps} to run on said hardware, with varying results.
\end{itemize}

\section{Future Directions}

While this dissertation has explored new areas, it also points to worthwhile future work for \gls{cps}, and creating successful implementations of high concurrency solutions to problems.  A number of the most promising lines of enquiry are described below.

% \subsection{`Faked' Message Passing in Shared Memory}
% Roughly, most of the message passing involved here is largely, in effect, just handing around pointers to memory locations.  It would seem that the message passing itself places some overhead in the way of that.  Could there be some way to fake the message passing, so that to the program's writer it looks like an actual message passing implementation, but in reality it is just doing normal updates on mutable memory?  This \emph{might} enable the best of both worlds -- programming the algorithms according to their theory, but running in a highly efficient fashion `under-the-hood'.

% None of the below were investigated further, due to a lack of time, but they are obvious next steps to look at.

% It is not too clear how to achieve this (if it is indeed possible), but Rust would appear to be a good language to target for it.  Rust is fairly high-performance by default and enables quite a lot of low-level memory manipulation.  Moreover, its move semantics pretty much fit exactly to the concepts used here, and, on the face of it, its macro system would appear to be a convenient way to abstract over many of the details and provide a message passing fa√ßade, while actually doing efficient operations behind that.

% Extended Dataflow Actors?  Or is it Extended Dataflow Architecture?

% See also the Bulk Synchronous Parallel approach.

% It looks like the C++ \texttt{mess} library is intended to be exactly this sort of thing:  \url{https://github.com/LouisCharlesC/mess}.  The developer seems to say that the user gets to write their program in a message-passing fashion, but mess does some clever meta-programming so that there ends up being zero overhead in the end.  Also, absolutely \emph{must} address Halide, and explain why it wasn't pursued here.  Otherwise, that'll be the elephant in the room.

% https://scholar.google.co.nz/scholar?q=related:Z8GZl-HQcSkJ:scholar.google.com/&scioq=A+Static+Mapping+System+for+Logically+Shared+Memory+Parallel+Programs&hl=en&as_sdt=0,5&inst=15360723290749679499

% http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.50.8739&rep=rep1&type=pdf

% https://link.springer.com/chapter/10.1007/BFb0057916

% https://link.springer.com/chapter/10.1007/3-540-63697-8_82

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Unification}

One-way multiset unification frequently occurs in \gls{cps}, with unification used in almost every rule presented \crefrange{chap:tsp}{chap:nmp}.  An efficient algorithm to perform this task would be highly beneficial for creating useful simulations of systems.  For example, the simulations of the \gls{tsp} algorithm written in functional programming languages (see \vref{sec:tsp:simulation}) regularly simply iterate over all relevant objects in the system, even though frequently most will be of little use in a given function call, and so the simulations could benefit from improved unification in practice.  At the time \cref{chap:tsp} was completed, no efficient algorithm for this task was known, but more recently \citeauthor{Liu2021} published \citetitle{Liu2021} \cite{Liu2021}.  Given that the \gls{tsp} simulations are near-direct translations from the \gls{cps} \glspl{ruleset} to sequential programs, they especially should be revisited in light of this development.  %  Given that they hew most closely to a direct translation from \gls{cps}, the \gls{tsp} simulations especially should be revisited in light of this development.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Universal Numbers}

The solutions and systems described in this work, when performing numerical tasks, always assumed the exclusive use of natural numbers.  For many tasks, this is adequate, but many other tasks require a workable approximation of real numbers, both positive and negative.  Currently, there is no well-defined method for representing and using arbitrary real numbers in most forms of \gls{ps}, including \gls{cps}.  If \gls{cps} is to be used to solve a broader range of problems, some form of representation for real numbers will eventually be required.

There seem to be two leading candidates for such representations:  simulating IEEE-754 floating-point numbers \cite{ieee754}, which are widely supported by most modern electronic computer hardware;  or, using the concept of \emph{universal numbers (unums)} advanced more recently by \citeauthor{Gustafson2017} \cite{Gustafson2017}.  Systematising working representations of one or the other seems likely to be highly beneficial to solving future problems.  On the surface, at least, unums appear to be a better operative fit with \gls{cps}, and so perhaps should be explored first.  IEEE-754 floating-point numbers are much better supported in current commodity hardware, however.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Neighbourhood Message Passing}
An obvious next step for the \gls{nmp} work is to adapt the system to the purpose of \gls{lbp} \gls{sm}, using \gls{nmp} precepts explicitly.  Time and space constraints prevented further work in this direction for this dissertation, however.

In the systems presented above, the size and shape of the grid and the communication topology between neighbours are permanently fixed at system initialisation.  Greater flexibility is usually not needed, but it might be useful in some circumstances, such as when a dynamic communication topology is necessary for a particular algorithm.  Modifying the systems from \cref{chap:nmp} accordingly and analysing the impacts on running computer programs is a clear potential extension to this paper's work.

At present, every \gls{pe} stays active throughout the system's evolution until it has sent and received all its scheduled messages.  Permitting \glspl{pe} to deactivate at appropriate points before they have reached their maximum generation count could save processing capabilities when the potential for parallelism is bounded.  As discussed with Conjecture \ref{conj:nmp:3}, however, any \gls{pe} which ceases messaging before reaching the maximum generation count can eventually affect the entire system.  Implementing effective early stopping is an unsolved problem.

These systems also have not been examined concerning communication complexity measures such as those found in \cite{Juayong2020}.  The precise results presented there are not directly applicable to this work, given the use of different \gls{ps} models, but the underlying concepts appear directly relevant.  Finding ways to define and quantify the communication complexity and `cost' (\cite{Juayong2020}'s \emph{ComR} and \emph{ComW}) seems particularly pertinent for determining the overall characteristics of a system.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Alternative Hardware}
This work focused on CPU-based systems, excluding other hardware.  Fundamentally, there is a mismatch between current (\circa{} 2021) CPUs and the underlying concept of \gls{cps}.  Specifically, current CPUs provide a handful of highly capable and flexible processors (at the time of writing, typically somewhere between 4 and 12 cores), while \gls{cps} essentially assumes the presence of an unbounded number of ``smaller'' processors.  This incongruence between the model and the hardware inevitably means attempting to use the latter to simulate the former will lead to unsatisfactory results.  Other hardware should be explored if the full power of \gls{cps} (and indeed \gls{ps} in general) is to be realised, because of the potential for large-scale parallelism that devices such as \glspl{gpu} and \glspl{fpga} offer.

Given the use of numerous smaller processors sounds remarkably close to the Single-Instruction Multiple-Thread (see \cite[Ch. 4.4.1]{Hennessy2012}) nature of modern \gls{gpgpu}, it would seem that \glspl{gpu} represent a clear next step for developing simulations further.  A \gls{gpu} implementation of the \gls{nmp} variants, in particular, seems like a promising target for further investigation.  Each \gls{pe} in a \gls{nmp} system likely has small processing requirements individually, but an entire system might have many \glspl{pe}.  The cores on a \gls{gpu} are limited in their capabilities compared to a CPU's cores, but there are many more of them, and modern \glspl{gpu} can handle processes with millions of live threads.  Thus, \gls{nmp} and \glspl{gpu} are potentially a superb match.

It is not yet apparent, however, how best to join execution of \gls{cps} rules with \gls{gpgpu}.  The high levels of messaging and control flow might not fit well with \gls{gpu} architecture and prove to be confounding factors, as \gls{gpu} performance is generally averse to branching.  Further work is required in this space to find an optimal strategy.  In fact, there is almost certainly more efficiency to be extracted from CPUs, too.  Given that it can be used fairly directly for CPUs, \glspl{gpu} and \glspl{fpga}, targeting translation of \gls{cps} \glspl{ruleset} to OpenCL\footnote{\url{https://www.khronos.org/opencl/}} may prove fruitful.

\Gls{mc} overall fundamentally assumes that the processors involved operate differently from current electronic computers.  Modern electronic computers follow a model in which the processor and the memory are distinct, and that data and instructions are shuttled back and forth between the two.\footnote{Indeed, making efficient use of the memory hierarchy tends to be critical to achieving high performance in modern application development.}  All the major forms of \gls{ps}, however, assume essentially that the memory storage \emph{is} the processor, whatever form the \glspl{compartment} take.  This differing theoretical operation suggests that non-standard hardware, such as logic modules and associative memory, may more directly support \gls{mc}.  Investigating this alternative hardware thus appears to be an important line of future research.  Novel approaches \emph{may} prove superior for systems such as \glspl{fpga} and semi-distributed systems like high-performance computing clusters, though that is largely speculation at this point.

% Researching uses of alternative hardware for \gls{nmp} would be worthwhile because of the potential for large-scale parallelism offered by devices such as \glspl{gpu} and \glspl{fpga}.  A \gls{gpu} implementation of the \gls{nmp} variants, in particular, seems like a promising target for further investigation.  Each \gls{pe} in a \gls{nmp} system likely has small processing requirements individually, but an entire system might have many \glspl{pe}.  The cores on a \gls{gpu} are limited in their capabilities compared to a CPU's cores, but there are many more of them, and modern \glspl{gpu} often comfortably run processes with millions of live threads.  Thus, \gls{nmp} and \glspl{gpu} are potentially a superb match.  The high levels of messaging and control flow, however, might not fit well with \gls{gpu} architecture, as \gls{gpu} performance is generally averse to branching.  Novel approaches \emph{may} prove superior for systems such as \glspl{fpga} and semi-distributed systems like high-performance computing clusters, though that is largely speculation at this point.

% It would appear worthwhile to investigate how to implement a high-performance message passing system atop these hardware alternatives, due to the potential for many more computations per second.  Even better would be to create a heterogeneous system which can take full advantage of the strengths of each hardware type, while overcoming its weaknesses.  Precisely how to achieve efficient implementations on them is unknown.  The fact that OpenCL (and, to some extent at least, OpenACC can be compiled from the same base code to different devices makes it an obvious starting point.%   \fxerror{Note to self}{There has already been at least one publication on implementing \glspl{actor} in OpenCL \fxerror[inline]{[ref]}, suggesting it is possible -- though how well synchronous message passing will work as compared to asynchronous remains to be seen.}

% The use of Full/Empty bits looks highly promising for making message passing more efficient.  It doesn't seem to have any real support in mainstream/commodity hardware, however.  There were Intel's TGX instructions, but they have been taken out of processors again.

% See also \url{https://hastlayer.com/} -- they seem to say that they do relevant stuff on \glspl{fpga}.  Interestingly, they also do unums/posits, apparently.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{\glsfmtname{plingua}}
Up to the time of writing, at least, \gls{cps} has often stood apart from many other \gls{ps} types in many ways.  For example, there is no representation for \gls{cps} in \gls{plingua}, nor support for simulating \gls{cps} in \gls{mecosim}.  There is no apparent reason that \gls{cps} cannot or should not be implemented in both, or successors to the two.  It seems worthwhile to develop a \gls{cps} representation in \gls{plingua} so that it can be used in the various tools and simulators based on \gls{plingua} (see \vref{sec:back:simulators}).  Support for \gls{cps} in \gls{mecosim} would make it much easier for researchers unversed in \gls{cps} to begin experimenting with it.
