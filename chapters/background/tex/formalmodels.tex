\section{\label{sec:back:formalmodels}Formal Models of Concurrent Computation}
The models briefly reviewed in this \namecref{sec:back:formalmodels} are just a small sample of the formal models for computation that have been devised.  An important aspect of each of the ones mentioned below is that they explicitly deal with concurrency in some fashion, thus meriting their (severely abridged) discussion.  Ultimately, \gls{mc} is used as the model of choice for the remainder of the dissertation after this \namecref{chap:back}, but it undoubtedly has been influenced by, or developed contemporaneously with, each of the below.  Indeed, close study of each model inevitably reveals commonalities between them.

\subsection{\label{subsec:back:csp}Communicating Sequential Processes}

\emph{\Gls{csp}} is a \emph{process algebra} and abstract model of concurrent computation put forward by Hoare \cite{Hoare1985,Roscoe2011}.  A typical sequential computation is represented by a \emph{process}.  Processes' \textcquote[][p.~478]{Roscoe2011}{behaviour is described in terms of the occurrence and availability of abstract entities called \textit{events}}.  Should more than one event be available simultaneously for a given process, then one will be chosen non-deterministically.  This choice is internal to the process, and not influenced by, or visible to, other processes.

Concurrency is introduced by the existence of multiple processes.  In general, the processes continue independently, responding to events as they come.  Should a particular event appear in the alphabet of multiple processes, however, then all processes \emph{must} choose to participate in that event at the same time.  Should all processes involved make such a choice, they engage in a synchronous multi-way atomic synchronisation (hence ``communicating'').  \gls{csp} has provided significant inspiration for concurrency design in a number of programming languages, notably including Ada \cite{Defense1983,Taft2013}, Occam \cite{Elizabeth1987}, Google's Go \cite{Meyerson2014} (not to be confused with the earlier language Go! \cite{Clark2004}, which itself was explicitly designed for concurrency) and \gls{cml} \cite{Reppy2011} (see further \vref{sec:back:cml}).  An especially interesting practical application of \gls{csp} itself is finding flaws -- and fixes to the flaws -- in information security protocols \cite{Roscoe1995,Lowe1996,Koltuksuz2010}.

\subsubsection{\(\pi\) Calculus}
\(\pi\) calculus was created by Robin Milner and associates in the early 1990s, building upon \gls{csp}.  Milner appreciated \gls{csp}, which advanced concurrency models by explicitly incorporating \emph{synchronised} interaction, something his earlier Calculus for Communicating Systems \cite{Milner1980} had lacked  \cite{Milner1993}.  Milner still regarded \gls{csp} as incomplete, however, in that it had no support for the concept of `mobility' --- \ie{} the ability of the system to reconfigure itself during operation.  In the context of \(\pi\) calculus, this is achieved by passing references to links between processes via other links, thus enabling a dynamic communication system. \(\pi\) calculus was created as an attempt to build upon earlier systems but present a complete calculus of concurrent computation, in much the same way that Lambda Calculus \cite{Barendregt1984} is a complete calculus for sequential computation.\footnote{Milner also noted that sequential computation is a special case of concurrent computation \cite{Milner1993}.}

Both \gls{csp} and \(\pi\) calculus are effective for modelling concurrent systems (see \eg{} \cite{Roscoe2011}).  They have a drawback, however, in that they tend to be very verbose.  Specifications of behaviour for processes are intertwined with the descriptions of processes' states.  They are effective for specifying a system formally, and verifying the behaviour of said system by defined reductions (see \eg{} \cite[Ch.~3.2]{Varela2013}), but for larger systems the notational burden can make it difficult to understand what each process can and will do.

\subsection{\label{subsec:back:actors}\texorpdfstring{\Glspl{actor}}{Actors}}
The \emph{\gls{actor}} model was introduced by Carl Hewitt \cite{Hewitt1973} and substantially further developed for practical programming use by Gul Agha and others \cite{Agha1986,Agha1997}.  Much like \gls{csp} \& its cousins, the \gls{actor} model is based around the concept of sequential, separate but communicating processes which exchange messages.  Again, the processes make decisions independently and proceed based on their communications.  A key difference, however, is that in the \gls{actor} model the message exchanges are \emph{asynchronous}.  Each \gls{actor} has its own \emph{mailbox}, and may send messages to other \glspl{actor} so long as it knows their identity (which is equivalent for this purpose to a concept of a postal address for the \gls{actor}), \emph{but does not wait at all for a response before proceeding}.  Moreover, for \glspl{actor}, the \emph{only} way to communicate with one another is to know the intended recipient's identity/address.  Logical shared memory is explicitly disallowed,\footnote{It is entirely possible in practice, of course, to implement an \gls{actor} system on a shared memory system.  The model expressly forbids any notion of a resource shared in any fashion besides \glspl{actor} requesting a result from another \gls{actor} who has exclusive access to the resource, however.} which prevents changing who an \gls{actor} communicates with simply by \eg{} changing the holder of a channel endpoint, but also means that actors can be distributed by a runtime without (directly) affecting the practical programming.

The \gls{actor} model is popular for concurrent programming, possibly owing to its intuitive concept.  The fact that communication is asynchronous makes \glspl{actor} much more suitable for modelling distributed systems without shared memory than \gls{csp} or similar --- \glspl{actor} can send messages and proceed without (necessarily) needing to wait for a response, instead continuing forward based on the messages they have received.  By contrast, a typical distributed system with synchronous communication would have prohibitive time costs, given the relative slow speed of typical links between distributed computers as compared to their capacity for local processing.  Many \gls{actor} systems have been implemented for different programming languages (\eg{} \cite{Varela2001,Srinivasan2008,Charousset2016,Bernstein2016}), and in fact it is a core component, and perhaps largely responsible for the success, of Erlang/OTP \cite{Armstrong2010,Armstrong2013,Vinoski2012}.  A relatively new language, Pony \cite{Clebsch2015,Clebsch2017}, takes this even further.

\Glspl{actor}, when used for non-trivial real-world software, have been criticised at times \eg{} \cite{Welsh2013,Stucchio2013}.  While some of the criticisms described are implementation-specific (relating to Akka, a Scala \gls{actor} library), a common thread is that \glspl{actor} do not compose well.  This has the negative consequence that it is difficult to combine an \gls{actor} with anything else to create a new abstraction, and can require extensive modifications in source code to make relatively simple logical changes.

\subsection[GAMMA, the Chemical Abstract Machine, and Join Calculus]{\(\Gamma\), the Chemical Abstract Machine, and Join Calculus}

\subsubsection{\(\Gamma\)}
\citeauthor{Banatre1993} \cite{Banatre1993}, described the \(\Gamma\) (\emph{GAMMA}) ``language'', which uses the multiset as its singular data structure.  The originators stated that their goal with \(\Gamma\) was to create a \enquote{a program structuring tool} focusing on \enquote{logical parallelism} (which seems to be their name for concurrency), rather than creating a working programming language.  Indeed, they acknowledged that translating multiset transformations into programs working on existing electronic computers of the time may prove challenging.  The principal aim was to provide a facility for program designers to state at a high-level the intended transformations, avoiding imposing any ordering on the processing and instead permitting as much concurrency as possible:  \textcquote[][p.~102]{Banatre1993}{our high level of abstraction eliminates unfortunate sequential biases.}  In essence, \(\Gamma\) takes an extreme declarative approach.

Perhaps the largest drawback of \(\Gamma\) is its lack of any form of central control.  All computation is performed through local operations, while \textcquote[][p.~108]{Banatre1993}{the only known solutions to certain problems rely on control decisions involving the whole state of the computation.}  The first problem of such that \citeauthor{Banatre1993} point to is the \gls{gcp} (which is solved in linear time in \cref{chap:gcol} in the present dissertation).  While such problems \emph{can} be solved, expressing the solutions would likely be more, rather than less, complicated than in other systems.  Another criticism that can be levelled at \(\Gamma\) is its lack of formality around its data structure.  Multisets are used, but there is no clear foundation underpinning them.  Rather, in \cite{Banatre1993} both integers and tuples are used as the set elements without any consideration of how they might be constructed or represented.  This is probably appropriate for the extreme high level of \(\Gamma\), but it could impede conversions of its programs to lower level languages more suited to working implementations.

% Of note, \citeauthor{Banatre1993} examine sorting as one of their motivating examples, and come to the conclusion that \enquote{the maximum of a set can be computed by performing the comparisons of the elements in any order}, which should enable highly concurrent sorting.  This dissertation reinforces that assertion, presenting constant-time sorting \gls{cps} rulesets in \cref{sec:median:sorting}.

\subsubsection{The Chemical Abstract Machine}
\citeauthor{Berry1992} devised the idea of the \emph{\gls{cham}} \cite{Berry1992}, directly inspired by the concepts from, and building upon, \(\Gamma\).  The parallels between the \gls{cham} and \gls{mc} are clear.  Indeed, the \gls{cham} includes a concept of \glspl{compartment} created by membranes.  Moreover, in his foundational exposition of \gls{mc} \cite{Paun2000}, \citeauthor{Paun2000} directly cites \(\Gamma\) \cite{Banatre1988} and \gls{cham} \cite{Berry1992}.

\(\Gamma\) had a loose notion of being rooted in chemistry, but \gls{cham} significantly adds to the rigour of the analogy, introducing concepts such as molecules composed of atoms, where a system is a collection of such molecules and may be heated or cooled to change the molecular composition.  Where \cite{Banatre1993} made clear the potential of multiset transformations for concurrency, \cite{Berry1992} adds formalisms that enable much more precise specifications of the process of the transformations, without sacrificing the extreme levels of concurrency possible.  The authors demonstrate this fact by showing that \glspl{cham} \enquote{can implement known models of concurrent computation}, including CCS \cite{Milner1980}, mobile processes \cite{Milner1991} and concurrent lambda calculus \cite{Boudol1989}.%  They further draw a distinction between the \gls{cham} and CSP, noting that \enquote{models of concurrent programming languages such as CSP are conceptually based on standard sequential machine models augmented with scheduling facilities, not on specific abstract machines.}

\subsubsection{Join Calculus}
% In some ways, join calculus -- created in the 1990s by \citeauthor{Fournet1996} \cite{Fournet1996,Fournet2002} and using the \gls{cham} as its underlying ``machine'' -- is the most similar of the models discussed in this \namecref{sec:back:formalmodels} to \gls{mc}.  Both work with multisets, are inspired by the interactions and movement of atoms, and define computations through rulesets.  Join calculus takes its inspiration directly from chemistry and chemical reactions, however, with no reference to biology.  Where evolution in \gls{mc} occurs within cells, reactions in join calculus take place in the \gls{cham}.

% Join calculus has proved fruitful for assisting in the design of new concepts in practical programming, including JoCaml \cite{Fournet2003}, Joinads \cite{Petricek2011}\footnote{See also \url{http://tryjoinads.org/docs/use/joins.html}.} and Reagents \cite{Turon2012}.  Despite this utility, however, join calculus seems largely to have been neglected by those outside the theoretical computer science and programming languages communities.

% Where the \gls{cham} was a development of \(\Gamma\), join calculus is in turn a development of the \gls{cham}.  Join calculus -- created in the 1990s by \citeauthor{Fournet1996} \cite{Fournet1996,Fournet2002} and using the \gls{cham} as its underlying ``machine'' (although, it actually originally targeted \(\pi\) Calculus before switching to the \gls{cham} as a much better fit for its core concepts \cite{Fournet2002}) -- is arguably the most similar of the models discussed in this \namecref{sec:back:formalmodels} to \gls{mc}.  \citeauthor{Fournet2002} specifically describe join calculus as \textcquote[][p.~269]{Fournet2002}{an attempt to provide language support for asynchronous, distributed, and mobile programming.}  One could perhaps consider it to be loosely an equivalent alternative to \gls{mc}, grounded in chemistry rather than biology.  There are clear common elements between join calculus and \gls{cps}, including heavy use of pattern matching and communication via channels.%  Both work with multisets, are inspired by the interactions and movement of atoms, and define computations through rulesets.  Join calculus takes its inspiration directly from chemistry and chemical reactions, however, with no reference to biology.  Where evolution in \gls{mc} occurs within cells, reactions in join calculus take place in the \gls{cham}.

Where the \gls{cham} was a development of \(\Gamma\), join calculus is in turn a development of the \gls{cham}.  Join calculus -- created in the 1990s by \citeauthor{Fournet1996} \cite{Fournet1996,Fournet2002} and using the \gls{cham} as its underlying ``machine'' (although, it actually originally targeted \(\pi\) Calculus before switching to the \gls{cham} as a much better fit for its core concepts \cite{Fournet2002}) -- is arguably the most similar in practice of all the models discussed in this \namecref{sec:back:formalmodels} to \gls{mc}.  There are clear common elements between join calculus and \gls{cps}, including heavy use of pattern matching and communication via channels.  Even its reaction rules closely resemble the typical style of rules found in most flavours of \gls{ps} (see \cref{sec:back:mc,chap:cpsystems}).%  One could perhaps consider it to be loosely an equivalent alternative to \gls{mc}, grounded in chemistry rather than biology.  There are clear common elements between join calculus and \gls{cps}, including heavy use of pattern matching and communication via channels.%  Both work with multisets, are inspired by the interactions and movement of atoms, and define computations through rulesets.  Join calculus takes its inspiration directly from chemistry and chemical reactions, however, with no reference to biology.  Where evolution in \gls{mc} occurs within cells, reactions in join calculus take place in the \gls{cham}.

\citeauthor{Fournet2002} specifically describe join calculus as \textcquote[][p.~269]{Fournet2002}{an attempt to provide language support for asynchronous, distributed, and mobile programming.}  Join calculus has proved fruitful for assisting in the design of new concepts in practical programming, including JoCaml \cite{Fournet2003} (essentially an implementation in OCaml of join calculus as a programming system), Joinads \cite{Petricek2011}\footnote{See also \url{http://tryjoinads.org/docs/use/joins.html}.} and Reagents \cite{Turon2012}.  Despite this utility, however, join calculus seems largely to have been neglected by those outside the theoretical computer science and programming languages communities and appears to have been entirely unused in more recent years.  Later work building on this dissertation could well benefit from an attempt to use join calculus, but it appears less likely to be immediately fruitful than using \gls{mc}.

\subsection[Brane and Kappa-Calculus]{Brane- and \(\kappa\)-Calculus}
\subsubsection{Brane Calculus}
Brane Calculus\footnote{\citeauthor{Cardelli2005} actually referred to it as Brane \emph{Calculi}, given that he saw it as a family of related systems, but Calculus is used here for consistency with the other described models.} might appear, \foreign{prima facie}, to be the closest relative to \gls{mc} of all the models presented in this \namecref{sec:back:formalmodels}.  The subtitle given to its exposition is \enquote{Interactions of Biological Membranes} \cite{Cardelli2005}.  Both models are undeniably grounded in concepts of cellular biology, with particular emphasis on the cell's membranes.  Both focus heavily on the nesting of those membranes, and movement of items among them.  In actuality they are dissimilar.

Brane Calculus is as much a system for modelling certain biological processes as it is a computational system.  Importantly, it means that the primitives chosen for it are relatively biologically realistic, and likely much more feasible to implement physically using biology.  While that has some clear potential benefits, it also tends to make even relatively simple systems somewhat difficult to follow.

Brane Calculus also treats its membranes somewhat differently.  In \gls{mc} (certainly \gls{clps}, at least), membranes are used primarily to separate regions of computation and avert undesired interactions between the contents of \glspl{compartment}.  In Brane Calculus, however, much of the processing occurs \emph{on} the membrane itself: \textcquote[][p.~257]{Cardelli2005}{Membranes are not just containers, they are coordinators and actives sites of major activity.}  Brane Calculus seems like a promising potential target for an ``intermediate language'' in between \gls{ps} and real biological implementations, but also seems much less useful for expressing the core essence of general computations than \gls{mc}.

\subsubsection{\(\kappa\)-calculus}

Similarly to Brane Calculus, \(\kappa\)-calculus \cite{Danos2004} was introduced as a formal representation of an aspect of cellular biology grounded in prior work in theoretical computer science on concurrent languages.  The focus in \(\kappa\)-calculus is predominantly upon protein interactions, rather than membranes as with Brane calculus: \textcquote[][p.~70]{Danos2004}{Our language idealizes protein-protein interactions, essentially as a particular kind of graph-rewriting}.  This protein focus is demonstrated in \cite{Danos2004} with an example describing part of the digestion process that occurs inside \textit{E.Coli} bacteria, the \enquote{lactose operon}.

The development of \(\kappa\)-calculus may have remained more firmly grounded in traditional concurrency theory than Brane Calculus.  Notably, \citeauthor{Danos2004}, in \textcquote[][p.~76]{Danos2004}{a side observation perhaps only of interest for Concurrency theorists} comment upon their use of \enquote{the ordinary material of process algebras}, pointing out that they use only a minimal subset of the usual operations.  Furthermore, they demonstrated that their simpler (in terms of allowed protein reactions) language \(m\kappa\)-calculus could simulate \(\pi\) calculus, meaning that they could translate systems created in \(\kappa\)-calculus to \(\pi\) calculus and then use programmatic implementations of \(\pi\) calculus, such as Nomadic Pict \cite{Unyapoth2001}, to run those systems on electronic computers.  While this shows that \(\kappa\)-calculus could be used to model solutions to arbitrary problems, it still suffers from the same problem as Brane Calculus.  The modelling for even relatively simple systems quickly becomes quite complex and difficult to read, diminishing the utility of \(\kappa\)-calculus for exploring alternative solutions to known problems in computer science.

\subsection{\label{sec:back:othermodels}Other Models}

\subsubsection{Petri Nets}
Perhaps the earliest formalised model of concurrent computation still in wide use is the \emph{Petri net} \cite{Dennis2011}, first conceived of by Carl Petri to describe chemical processes \cite{Petri2008}.  A basic Petri net consists of places, tokens and transitions, where tokens move between places via arcs and all places are separated by transitions (and \viceversa).

The main weakness of classical Petri nets is that they require the definition of a fixed system from the outset, and do not easily permit the evolution of the system during ``runtime'' as appropriate.  Of Petri nets, \citeauthor{Varela2013} says \textcquote[][p.~36]{Varela2013}{Petri nets \textelp{} in the \textelp{} form described \textins{in \cite{Varela2013}}, \textelp{} are not able to model the dynamicity of concurrent software\textins{.}  \textelp{} Petri nets \textelp{} do not support the compositional reasoning afforded by more modern models of concurrency.}

\subsubsection{\label{sec:back:pram}Parallel Random Access Machines}

\emph{\Glspl{pram}} are a model for concurrent computation arguably closer to the operation of real electronic computers than the others described above.  \citeauthor{JaJa2011} defines a \gls{pram} as \textcquote{JaJa2011}{an abstract model for parallel computation which assumes that all the processors operate synchronously under a single clock and are able to randomly access a large shared memory. In particular, a processor can execute an arithmetic, logic, or memory access operation within a single clock cycle}.  Unlike the process algebras described above, the \gls{pram} model does not, in general, impose a particular way to describe a \gls{pram} algorithm, making it sometimes quite seful in developing parallel algorithms.

\Glspl{pram} developed as a natural extension to random access machines (RAMs), which, as the name suggests, provide a model that is reasonably close to typical uniprocessor electronic computers.  A \gls{pram} is a collection of processors operating synchronously (\ie{} on the same clock), each labelled with a unique identifier, and with access to a global shared memory.  Processors may operate independently of each other, but many described \gls{pram} algorithms in fact operate in a \emph{single-program multiple-data} fashion where all processors follow the same program and so are only semi-independent, at best.  These attributes mean that a \gls{pram} often looks quite similar (at a superficial level, at least) to the operation of a typical \gls{gpu}.  The systems described in \cref{chap:nmp} also bear more than a passing resemblance to \glspl{pram}.

% \subsubsection{Communicating Reactive Processes}