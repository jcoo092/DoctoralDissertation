\section{Experimental results}

To validate \autoref{ruleset:nmp:proxspec} and investigate the properties of the \gls{pe}-specific approach to \gls{nmp}, we have performed some small experiments with short prototype programs.  In particular, we have:  (i) simulated the operation of the ruleset without actually passing messages, to verify empirically that Theorem~\ref{theorem:nmp:-4} holds for an arbitrary \gls{pe};  (ii) implemented and benchmarked three variants of the asynchronous \gls{nmp} algorithm operating on a \gls{fne} grid, to assess whether the fully asynchronous version genuinely completes faster on average; (iii) investigated how convergent the variants are; and, (iv) compared the comparative timings of the variants for when the first, `average' and last \gls{pe} finish their processing.  Source code for the described scripts or programs may be found at \url{https://github.com/jcoo092/NeighbourhoodMessagePassingExperiments}.

The \gls{cps} models proposed were extremely helpful as a starting point for developing the experimental programs.  The models led fairly naturally and directly to the implementations, with most subsequent development to fit the model better to the host language's features.  The converse is also true.  The process of implementing the programs uncovered a handful of problems in earlier drafts of the rules.

One of the more significant examples of the benefit comes from early in the development.  As it was at the time, the validation program would usually end normally but occasionally crashed or entered an infinite loop.  Investigating this issue's cause revealed a potential race condition in \autoref{ruleset:nmp:proxspec} (as it was then).  It was possible for \glspl{pe} either to send too few or too many messages.

\subsection{Validation of rules for asynchronous \texorpdfstring{\gls{nmp}}{neighbourhood message passing}}
A short script was written in \fsharp{}.  This script takes the perspective of a single arbitrary \gls{pe} on a \gls{fne} grid and simulates the receipt and sending of messages.  Actual data are irrelevant to this test, so message data \gls{oq} updates were omitted.  During each round, the \gls{pe} `receives messages' from one or more randomly selected neighbours.  Said receipts are represented by incrementing the received-message generation counts and creating receipt tokens.  The \gls{pe} then generates \gls{nm} messages per \autoref{ruleset:nmp:proxspec} based on the generation counts and receipt tokens and increments the sent-message counts as appropriate.  Finally, the script prints the results then terminates once messaging completes.

Inspecting the state of the \gls{pe} at various points in the system's execution confirmed \autoref{theorem:nmp:-4}.  If exactly \(g\) messages were received from each neighbour, where \(g\) is the maximum generation count, the same number of messages would consequently be sent.  Furthermore, messages were indeed sent based on the ordering of messages received, as expected.

\subsection{\label{sec:nmp:timingexp}Timing of variations}
A self-contained command-line program was written in C\# to investigate the comparative running time of each of the three variants described above using the \gls{fne} grid.  Each \gls{pe} is represented by a separate asynchronous \texttt{Task} from .NET's Task Parallel Library{\footnote{\raggedright\url{https://docs.microsoft.com/en-us/dotnet/standard/parallel-programming/task-parallel-library-tpl}}}, with communication between \glspl{pe} conducted via a channel construct provided by .NET.\footnote{\url{https://docs.microsoft.com/en-us/dotnet/api/system.threading.channels}}  All three approaches are included in the same program, and selected via a runtime configuration file.

Multiple variables besides the variant were used over different runs to explore the relative behaviour of each variant in terms of running time.  These included:  the size of the grid; the maximum generation count;  the use and length of a work-intensive wait period when computing a new message to send (essentially, the \gls{oq} phase);  the use and length of delays in delivery of messages over the channels while permitting the \glspl{pe} to continue operating, as well as using different delay lengths for different directions.

Numerical results are presented in Tables \ref{tab:nmp:simulation8cores}-\ref{tab:nmp:simulation48cores}.  Running time results were gathered using the \texttt{Stopwatch} class from .NET, which provides access to underlying high-performance operating system time measurement facilities.  The program is instrumented to start the timer immediately before the initialisation of the \glspl{pe}, but \emph{after} reading and parsing the configuration file.  The timer is then stopped immediately after the last \gls{pe} finishes running, when control flow returns to the program's \texttt{main} function.  The total elapsed milliseconds are then printed to the command line.  The figures presented in the tables are the mean (rounded to the nearest whole number) of five runs for each variant under each parameter set.

In each case, the results are from running the program with a maximum generation count of 20 and an oracle delay of approximately one-quarter of a millisecond per message.  The delays over channels were varied between equal delays of 30 milliseconds, three channels with 30 milliseconds and one with 150 milliseconds, and two channels with 30 milliseconds and two with 150 milliseconds.  The test program was run on three different computers with three different CPUs, each with a different number of physical \& logical cores.  The CPUs were an Intel\textsuperscript{\textregistered} Core\textsuperscript{\texttrademark} i7-7700, with 4 physical/8 logical cores; an Intel\textsuperscript{\textregistered} Core\textsuperscript{\texttrademark} i7-8750H with 6 physical/12 logical cores; and an Intel\textsuperscript{\textregistered} Xeon\textsuperscript{\textregistered} Silver 4116 with 24 physical/48 logical cores.

\begin{table}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}r|rrr|rrr|rrr@{}}
\toprule
\multicolumn{1}{c|}{\# of}   & \multicolumn{3}{c|}{Equal delays} & \multicolumn{3}{c|}{One longer delay} & \multicolumn{3}{c}{Two longer delays} \\ \cmidrule(l){2-10} 
\multicolumn{1}{c|}{Proxels} & Global    & Local     & Async     & Global      & Local      & Async      & Global      & Local      & Async      \\ \midrule
361  & 1,553  & 1,212  & 1,212  & 4,029  & 3,226  & 3,104  & 4,026  & 3,514  & 3,516  \\
9,801  & 21,012  & 20,442  & 18,865  & 25,976  & 23,281  & 21,311  & 24,282  & 21,410  & 19,679  \\
89,401  & 192,730  & 192,329  & 174,984  & 225,454  & 219,261  & 200,889  & 222,104  & 216,537  & 200,637  \\
249,001  & 542,285  & 546,213  & 493,311  & 625,959  & 595,861  & 527,898  & 614,079  & 564,406  & 558,970 \\ \bottomrule
\end{tabular}%
}
\caption{Mean recorded running times in milliseconds for each \gls{nmp} variant in a simulation, with different sending delay lengths, on a computer with a CPU with 4/8 physical/logical cores}
\label{tab:nmp:simulation8cores}
\end{table}

\begin{table}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}r|rrr|rrr|rrr@{}}
\toprule
\multicolumn{1}{c|}{\# of}   & \multicolumn{3}{c|}{Equal delays} & \multicolumn{3}{c|}{One longer delay} & \multicolumn{3}{c}{Two longer delays} \\ \cmidrule(l){2-10} 
\multicolumn{1}{c|}{Proxels} & Global    & Local     & Async     & Global      & Local      & Async      & Global      & Local      & Async      \\ \midrule
361  & 1,457  & 1,186  & 1,148  & 3,875  & 3,229  & 3,096  & 3,918  & 3,471  & 3,493  \\
9,801  & 15,865  & 15,116  & 14,290  & 18,392  & 15,295  & 14,564  & 18,389  & 15,267  & 14,612  \\
89,401  & 145,547  & 141,024  & 133,509  & 149,368  & 141,811  & 134,382  & 148,439  & 141,619  & 134,289  \\
249,001  & 408,463  & 392,901  & 371,576  & 413,106  & 391,907  & 370,683  & 413,224  & 394,115  & 372,556 \\ \bottomrule
\end{tabular}%
}
\caption{Mean recorded running times in milliseconds for each \gls{nmp} variant in a simulation, with different sending delay lengths, on a computer with a CPU with 6/12 physical/logical cores}
\label{tab:nmp:simulation12cores}
\end{table}

\begin{table}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}r|rrr|rrr|rrr@{}}
\toprule
\multicolumn{1}{c|}{\# of}   & \multicolumn{3}{c|}{Equal delays} & \multicolumn{3}{c|}{One longer delay} & \multicolumn{3}{c}{Two longer delays} \\ \cmidrule(l){2-10} 
\multicolumn{1}{c|}{Proxels} & Global    & Local     & Async     & Global      & Local      & Async      & Global      & Local      & Async      \\ \midrule
361  & 1,370  & 1,248  & 1,122  & 3,710  & 3,204  & 3,091  & 3,741  & 3,436  & 3,419  \\
9,801  & 9,986  & 9,060  & 7,571  & 12,012  & 9,549  & 8,007  & 11,932  & 9,392  & 7,911  \\
89,401  & 102,718  & 92,817  & 76,583  & 106,166  & 96,293  & 78,341  & 106,984  & 94,060  & 78,025  \\
249,001  & 296,516  & 255,703  & 215,504  & 299,926  & 265,365  & 215,721  & 298,732  & 263,814  & 216,048 \\ \bottomrule
\end{tabular}%
}
\caption{Mean recorded running times in milliseconds for each \gls{nmp} variant in a simulation, with different sending delay lengths, on a computer with a CPU with 24/48 physical/logical cores}
\label{tab:nmp:simulation48cores}
\end{table}

The data for Tables \ref{tab:nmp:simulation8cores}-\ref{tab:nmp:simulation48cores} are visualised in Figures \ref{fig:nmp:timings8cores}-\ref{fig:nmp:timings48cores}, with the variants on the x-axis and milliseconds on the y-axis.  Each figure has four bar charts for the four different grid sizes used.  Each chart compares the \gls{gs}, \gls{ls} and asynchronous variants, both against each other and between the different number of longer delays used.  In every figure, the chart layout for the grid sizes (in number of \glspl{pe}) is as follows:  top-right, 361;  top-left, 9,801;  bottom-left, 89,401;  bottom-right, 249,001.  In each chart, the bar clusters are, from left to right:  the timings for the experiments when there were equal delays on all channels, the experiments when there was a longer delay on one channel, and the experiments where there were longer delays on two channels.  Within each bar cluster, in every instance, the bars represent the timings for (from left to right) the \gls{gs}, \gls{ls} and asynchronous variants.

\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{groupplot}[ %361 proxels
            group style={
                group size=2 by 2,
                vertical sep=0.70cm,
                xlabels at=edge bottom,
                xticklabels at=edge bottom,
                ylabels at=edge left,
            },
            enlargelimits=0.25,
            height=0.50\textheight,
            % legend columns=-1,
            % legend entries={\Gls{gs}, \Gls{ls}, Asynchronous},
            % legend to name=timingsleg,
            symbolic x coords={Equal,One,Two},
            small,
            width=0.53\textwidth,
            xlabel=Number of longer delays,
            xtick={data},
            ybar,
            ylabel=Milliseconds,
        ]
            % 361
            \nextgroupplot
            \addplot %global
            coordinates {
                (Equal,1553) (One,4029) (Two,4026)   
            };
            \addplot %local
            coordinates {
                (Equal,1212) (One,3226) (Two,3514)   
            };
            \addplot %async
            coordinates {
                (Equal,1212) (One,3104) (Two,3516)   
            };
            
            % 9,801
            \nextgroupplot
            \addplot %global
            coordinates {
                (Equal,21012) (One,25976) (Two,24282)   
            };
            \addplot %local
            coordinates {
                (Equal,20442) (One,23281) (Two,21410)   
            };
            \addplot %async
            coordinates {
                (Equal,18865) (One,21311) (Two,19679)   
            };
            
            % 89,401
            \nextgroupplot
            \addplot %global
            coordinates {
                (Equal,192730) (One,225454) (Two,222104)   
            };
            \addplot %local
            coordinates {
                (Equal,192329) (One,219261) (Two,216537)   
            };
            \addplot %async
            coordinates {
                (Equal,174984) (One,200889) (Two,200637)   
            };
            
            % 249,001
            \nextgroupplot
            \addplot %global
            coordinates {
                (Equal,542285) (One,625959) (Two,614079)   
            };
            \addplot %local
            coordinates {
                (Equal,546213) (One,595861) (Two,564406)   
            };
            \addplot %async
            coordinates {
                (Equal,493311) (One,527898) (Two,558970)   
            };
        \end{groupplot}
    \end{tikzpicture}
    % \ref{timingsleg}
    \caption{Bar chart visualising the timing differences for the variants running on a CPU with 8 cores, with different sized grids and differing channel delay lengths.  Each bar cluster goes from left to right with the \gls{gs}, \gls{ls} and asynchronous variants respectively.  The numbers of \glspl{pe} are:  Top-left: 361;  Top-right:  9,801;  Bottom-left:  89,401;  Bottom-right:  249,001.  See also \autoref{tab:nmp:simulation8cores}}
    \label{fig:nmp:timings8cores}
\end{figure}

\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{groupplot}[
            group style={
                group size=2 by 2,
                vertical sep=0.70cm,
                xlabels at=edge bottom,
                xticklabels at=edge bottom,
                ylabels at=edge left,
            },
            enlargelimits=0.25,
            height=0.50\textheight,
            symbolic x coords={Equal,One,Two},
            small,
            width=0.53\textwidth,
            xlabel=Number of longer delays,
            xtick={data},
            ybar,
            ylabel=Milliseconds,
        ]
            % 361
            \nextgroupplot
            \addplot %global
            coordinates {
                (Equal,1457) (One,3875) (Two,3918)   
            };
            \addplot %local
            coordinates {
                (Equal,1186) (One,3229) (Two,3471)   
            };
            \addplot %async
            coordinates {
                (Equal,1148) (One,3096) (Two,3493)   
            };
            
            % 9,801
            \nextgroupplot
            \addplot %global
            coordinates {
                (Equal,15865) (One,18392) (Two,18389)   
            };
            \addplot %local
            coordinates {
                (Equal,15116) (One,15295) (Two,15267)   
            };
            \addplot %async
            coordinates {
                (Equal,14290) (One,14564) (Two,14612)   
            };
            
            % 89,401
            \nextgroupplot
            \addplot %global
            coordinates {
                (Equal,145547) (One,149368) (Two,148439)   
            };
            \addplot %local
            coordinates {
                (Equal,141024) (One,141811) (Two,141619)   
            };
            \addplot %async
            coordinates {
                (Equal,133509) (One,134382) (Two,134289)   
            };
            
            % 249,001
            \nextgroupplot
            \addplot %global
            coordinates {
                (Equal,408463) (One,413106) (Two,413224)   
            };
            \addplot %local
            coordinates {
                (Equal,392901) (One,391907) (Two,394115)   
            };
            \addplot %async
            coordinates {
                (Equal,371576) (One,370683) (Two,372556)   
            };
        \end{groupplot}
    \end{tikzpicture}
    % \ref{timingsleg}
    \caption{Bar chart visualising the timing differences for the variants running on a CPU with 12 cores, with different sized grids and differing channel delay lengths.  Each bar cluster goes from left to right with the \gls{gs}, \gls{ls} and asynchronous variants respectively.  The numbers of \glspl{pe} are:  Top-left: 361;  Top-right:  9,801;  Bottom-left:  89,401;  Bottom-right:  249,001.  See also \autoref{tab:nmp:simulation12cores}}
    \label{fig:nmp:timings12cores}
\end{figure}

\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{groupplot}[ %361 proxels
            group style={
                group size=2 by 2,
                vertical sep=0.70cm,
                xlabels at=edge bottom,
                xticklabels at=edge bottom,
                ylabels at=edge left,
            },
            enlargelimits=0.25,
            height=0.50\textheight,
            symbolic x coords={Equal,One,Two},
            small,
            width=0.53\textwidth,
            xlabel=Number of longer delays,
            xtick={data},
            ybar,
            ylabel=Milliseconds,
        ]
            % 361
            \nextgroupplot
            \addplot %global
            coordinates {
                (Equal,1370) (One,3710) (Two,3741)   
            };
            \addplot %local
            coordinates {
                (Equal,1248) (One,3204) (Two,3436)   
            };
            \addplot %async
            coordinates {
                (Equal,1122) (One,3091) (Two,3419)   
            };
            
            % 9,801
            \nextgroupplot
            \addplot %global
            coordinates {
                (Equal,9986) (One,12012) (Two,11932)   
            };
            \addplot %local
            coordinates {
                (Equal,9060) (One,9549) (Two,9392)   
            };
            \addplot %async
            coordinates {
                (Equal,7571) (One,8007) (Two,7911)   
            };
            
            % 89,401
            \nextgroupplot
            \addplot %global
            coordinates {
                (Equal,102718) (One,106166) (Two,106984)   
            };
            \addplot %local
            coordinates {
                (Equal,92817) (One,96293) (Two,94060)   
            };
            \addplot %async
            coordinates {
                (Equal,76583) (One,78341) (Two,78025)   
            };
            
            % 249,001
            \nextgroupplot
            \addplot %global
            coordinates {
                (Equal,296516) (One,299926) (Two,298732)   
            };
            \addplot %local
            coordinates {
                (Equal,255703) (One,265365) (Two,263814)   
            };
            \addplot %async
            coordinates {
                (Equal,215504) (One,215721) (Two,216048)   
            };
        \end{groupplot}
    \end{tikzpicture}
    % \ref{timingsleg}
    \caption{Bar chart visualising the timing differences for the variants running on a CPU with 48 cores, with different sized grids and differing channel delay lengths.  Each bar cluster goes from left to right with the \gls{gs}, \gls{ls} and asynchronous variants respectively.  The numbers of \glspl{pe} are:  Top-left: 361;  Top-right:  9,801;  Bottom-left:  89,401;  Bottom-right:  249,001.  See also \autoref{tab:nmp:simulation48cores}}
    \label{fig:nmp:timings48cores}
\end{figure}

There appear to be only two consistent trends in these data.  Firstly, in almost all cases, the asynchronous approach was the fastest, although, in some instances, it was approximately the same as the \gls{ls} case.  Secondly, an increase in the number of cores available appears to favour the asynchronous case.  While in all cases the running time decreased with an increase in the core count, the percentage decrease in running time was always greater for the asynchronous case than the \gls{gs} case, when comparing program execution run time with the same parameters between the 4/8 and 24/48 core CPUs.  We suspect that this latter trend means that the asynchronous version is more scalable with respect to the count of CPU cores but have not investigated this thoroughly yet.

Ultimately, however, there will likely be some upper limit on the improvement in running time offered by the asynchronous version.  The dependence upon the receipt of messages for every other neighbour before the preparation of a new outgoing message for the final neighbour ensures that any given \gls{pe} can only get so far ahead of its neighbours before it must wait for them.

The smallest performance difference on the same test between the \gls{gs} and asynchronous cases was on the 6/12 core CPU on a 299×299 \gls{pe} grid and with equal delays on all channels, where the \gls{gs} approach took only approximately 9\% longer than the asynchronous version.  The largest performance difference between the \gls{gs} and asynchronous versions was on the 24/48 cores CPU on a 99×99 \gls{pe} grid and with two longer delays on the channels, where the \gls{gs} approach took roughly 51\% longer.  The precise reason why these were the relative fastest and slowest test runs is unclear.  Across each CPU, the widest differences are seen when using a 99×99 \gls{pe} grid, or a total of 9,801 \glspl{pe}.  Perhaps this is closest to a `sweet spot' where the .NET runtime can best schedule blocked Tasks on each core without becoming overwhelmed by overheads related to their management.

Comparing the globally- and \gls{ls} cases, there is only one experiment where the mean running time for the \gls{ls} variant was higher than for the \gls{gs} case:  The 499×499 grid with equal delays between all channels, running on the 4/8 core CPU.  Even this is close, with only a 3.2\% running time increase.   On many of the other tests, the \gls{ls} version produces a \emph{decrease} in running time of roughly 5-13\%, yet still produces the same answer to the computation at hand (see \autoref{sec:nmp:convergence}).  On this basis, it would appear that the \gls{ls} approach could be regarded as strictly superior to and should be favoured over the \gls{gs} variant.

\subsection{\label{sec:nmp:convergence}Convergence}
Both synchronous variants will produce the same final computed result because the ordering of messages varies only within a single generation, and thus the inputs used to compute new messages will be the same at each round.  As delineated in Conjecture \ref{conj:nmp:1}, we also hypothesise that the asynchronous system will tend to produce comparable results to the other two.  The test program used in \autoref{sec:nmp:timingexp} was further instrumented to provide output at the end describing the state of each \gls{pe} in the grid to gather evidence regarding these hypotheses.

The grid was divided into quadrants, and each \gls{pe} assigned its own internal value.  \Glspl{pe} in the upper-left quadrant were initialised with a value of 0.0, those in the lower-left and upper-right received 0.5, and those in the lower-right received 1.0.  The value to send out in the next message to each neighbour was computed by averaging the values received from the other three neighbours plus the \gls{pe}'s own internal value.  At the end of each round, \glspl{pe} updated their internal values to the mean of the values most-recently-received from each neighbour (this bears some resemblance to the formula used for \autoref{fig:nmp:mean}).  %At the end of each round, the \gls{pe}'s internal value was updated to the mean of the values most-recently-received from each neighbour (this bears some resemblance to the formula used for \autoref{fig:nmp:mean}).

Once messaging finished, every \gls{pe} wrote its ending internal value to a file.  At this point, the values were rounded to two decimal places to account for the inherent imprecision of typical (e.g., those of IEEE standard 754 \cite{ieee754,Goldberg1991}) floating-point numbers.  These values were then extracted and compared between runs of variants on the same input configuration.

\subsubsection{Hamming distance}
To provide a quick measure of the level of difference between variants, a Hamming distance was computed between \glspl{pe} in the same grid position.  Matched \glspl{pe} with the same value were assigned 0, and matched \glspl{pe} with different values were assigned 1.  The total distance between the two sequences was the total of the assigned values.  I.e., the total distance was computed as \[d = \sum_{i = 1}^{i \leq n} a_i \oplus b_i\text{,}
\hspace{1.0cm}%
a_i \oplus b_i = \begin{cases}
    0, & \text{if } a_i = b_i \\
    1, & \text{otherwise}
\end{cases}
\] where \(d\) is the Hamming distance, \(n\) is the total number of \glspl{pe} in the grid, \(a_i\) is the \(i^{\text{th}}\) entry in one of the sequences, and \(b_i\) is the \(i^{\text{th}}\) entry in the other sequence.  An average distance between variants was then calculated by dividing the sum by \(n\).  This average provides a rough measure of the level of difference in the final results between variants and is equal to the percentage of the grid's \glspl{pe} with different results between variants.

\paragraph{\Gls{gs} vs \gls{ls}}
To check that the \gls{gs} and \gls{ls} variants produce the same final result for every \gls{pe}, the results from the two variants were compared using the above formula.  In every instance, the total distance was 0, meaning there was no difference whatsoever.  This appears to confirm that the \gls{gs} and \gls{ls} variants always produce the same final result.

\paragraph{\Gls{ls} vs asynchronous}
Knowing that the \gls{gs} and \gls{ls} variants produce the same result, the asynchronous variant was compared against only the \gls{ls} one.  In all cases, the runs with equal delays on all channels produced the smallest distances.  In most cases, the runs with a single unbalanced channel delay produced the largest differences, sometimes more than double that of the runs with two unbalanced channels.  Tables \ref{tab:nmp:hamming8cores}-\ref{tab:nmp:hamming48cores} list the total distance scores, and the scores expressed as a percentage of the total \gls{pe} count.

\begin{table}
\centering
\begin{tabular}{@{}r|rr|rr|rr@{}}
\toprule
\multicolumn{1}{c|}{\# of}   & \multicolumn{2}{c|}{Equal delays} & \multicolumn{2}{c|}{One longer delay} & \multicolumn{2}{c}{Two longer delays} \\ \cmidrule(l){2-7} 
\multicolumn{1}{c|}{Proxels} & Distance     & Percentage     & Distance      & Percentage      & Distance      & Percentage      \\ \midrule
361  & 31  & 8.70\% & 233  & 64.49\% & 88  & 24.32\% \\
9,801  & 76  & 0.77\% & 401  & 4.09\% & 220  & 2.24\% \\
89,401  & 359  & 0.40\% & 638  & 0.71\% & 607  & 0.68\% \\
249,001  & 898  & 1.00\% & 1,103  & 1.23\% & 1,454  & 1.63\% \\ \bottomrule
\end{tabular}%
% }
\caption{Mean Hamming distances for \gls{pe} ending values between the \gls{ls} and asynchronous variants in a simulation, with different sending delay lengths, on a computer with a CPU with 4/8 physical/logical cores}
\label{tab:nmp:hamming8cores}
\end{table}  

\begin{table}
\centering
\begin{tabular}{@{}r|rr|rr|rr@{}}
\toprule
\multicolumn{1}{c|}{\# of}   & \multicolumn{2}{c|}{Equal delays} & \multicolumn{2}{c|}{One longer delay} & \multicolumn{2}{c}{Two longer delays} \\ \cmidrule(l){2-7} 
\multicolumn{1}{c|}{Proxels} & Distance     & Percentage     & Distance      & Percentage      & Distance      & Percentage      \\ \midrule
361  & 25  & 6.81\% & 238  & 65.82\% & 74  & 20.39\% \\
9,801  & 83  & 0.85\% & 557  & 5.68\% & 330  & 3.37\% \\
89,401  & 277  & 0.31\% & 335  & 0.37\% & 431  & 0.48\% \\
249,001  & 586  & 0.66\% & 837  & 0.94\% & 1,276  & 1.43\% \\ \bottomrule
\end{tabular}%
% }
\caption{Mean Hamming distances for \gls{pe} ending values between the \gls{ls} and asynchronous variants in a simulation, with different sending delay lengths, on a computer with a CPU with 6/12 physical/logical cores}
\label{tab:nmp:hamming12cores}
\end{table}  

\begin{table}
\centering
\begin{tabular}{@{}r|rr|rr|rr@{}}
\toprule
\multicolumn{1}{c|}{\# of}   & \multicolumn{2}{c|}{Equal delays} & \multicolumn{2}{c|}{One longer delay} & \multicolumn{2}{c}{Two longer delays} \\ \cmidrule(l){2-7} 
\multicolumn{1}{c|}{Proxels} & Distance     & Percentage     & Distance      & Percentage      & Distance      & Percentage      \\ \midrule
361  & 42  & 11.69\% & 248  & 68.75\% & 116  & 32.08\% \\
9,801  & 177  & 1.80\% & 1,422  & 14.51\% & 581  & 5.93\% \\
89,401  & 152  & 0.17\% & 243  & 0.27\% & 229  & 0.26\% \\
249,001  & 221  & 0.25\% & 367  & 0.41\% & 332  & 0.37\% \\ \bottomrule
\end{tabular}%
% }
\caption{Mean Hamming distances for \gls{pe} ending values between the \gls{ls} and asynchronous variants in a simulation, with different sending delay lengths, on a computer with a CPU with 24/48 physical/logical cores}
\label{tab:nmp:hamming48cores}
\end{table}

As expected, smaller grids produce smaller total distances but greater scores as a percentage of the total grid size.  Interestingly, the proportionally lowest scores in all cases were seen on the 299×299 grids.  On the smaller grids, the CPUs with more cores tended to have higher distances but smaller distances on the larger grids.  We are yet to find a convincing explanation for the latter two observations.

\subsubsection{Absolute difference}
The Hamming distance gives a clear indication of the proportion of the grid which computed a different result under the \gls{ls} and asynchronous variants but might under- or over-estimate the significance of those differences.  Using the same output empirical data, the distance was recomputed as the sum of the absolute difference in final \gls{pe} values between variants, i.e., \( d = \sum_{i = 1}^{i \leq n} |a_i - b_i| \), where \(n\), \(a_i\) and \(b_i\) are the same as above, and \(d\) is the new distance measure.

\begin{table}
\centering
\begin{tabular}{@{}r|rr|rr|rr@{}}
\toprule
\multicolumn{1}{c|}{\# of}   & \multicolumn{2}{c|}{Equal delays} & \multicolumn{2}{c|}{One longer delay} & \multicolumn{2}{c}{Two longer delays} \\ \cmidrule(l){2-7} 
\multicolumn{1}{c|}{Proxels} & Distance     & Percentage     & Distance      & Percentage      & Distance      & Percentage      \\ \midrule
361  & 0.314 & 0.150\% & 2.546 & 1.218\% & 0.878 & 0.420\% \\
9,801  & 0.758 & 0.008\% & 4.008 & 0.041\% & 2.196 & 0.022\% \\
89,401  & 3.586 & 0.004\% & 6.380 & 0.007\% & 6.072 & 0.007\% \\
249,001  & 8.984 & 0.004\% & 11.032 & 0.004\% & 14.544 & 0.006\% \\ \bottomrule
\end{tabular}%
% }
\caption{Mean sums of absolute differences for \gls{pe} ending values between the \gls{ls} and asynchronous variants in a simulation, with different sending delay lengths, on a computer with a CPU with 4/8 physical/logical cores}
\label{tab:nmp:diffs8cores}
\end{table}  

\begin{table}
\centering
\begin{tabular}{@{}r|rr|rr|rr@{}}
\toprule
\multicolumn{1}{c|}{\# of}   & \multicolumn{2}{c|}{Equal delays} & \multicolumn{2}{c|}{One longer delay} & \multicolumn{2}{c}{Two longer delays} \\ \cmidrule(l){2-7} 
\multicolumn{1}{c|}{Proxels} & Distance     & Percentage     & Distance      & Percentage      & Distance      & Percentage      \\ \midrule
361  & 0.246 & 0.118\% & 2.712 & 1.297\% & 0.736 & 0.352\% \\
9,801  & 0.832 & 0.017\% & 5.570 & 0.110\% & 3.304 & 0.065\% \\
89,401  & 2.770 & 0.006\% & 3.348 & 0.007\% & 4.306 & 0.010\% \\
249,001  & 5.860 & 0.005\% & 8.366 & 0.007\% & 12.762 & 0.010\% \\ \bottomrule
\end{tabular}%
% }
\caption{Mean sums of absolute differences for \gls{pe} ending values between the \gls{ls} and asynchronous variants in a simulation, with different sending delay lengths, on a computer with a CPU with 6/12 physical/logical cores}
\label{tab:nmp:diffs12cores}
\end{table}  

\begin{table}
\centering
\begin{tabular}{@{}r|rr|rr|rr@{}}
\toprule
\multicolumn{1}{c|}{\# of}   & \multicolumn{2}{c|}{Equal delays} & \multicolumn{2}{c|}{One longer delay} & \multicolumn{2}{c}{Two longer delays} \\ \cmidrule(l){2-7} 
\multicolumn{1}{c|}{Proxels} & Distance     & Percentage     & Distance      & Percentage      & Distance      & Percentage      \\ \midrule
361  & 0.422 & 0.202\% & 2.948 & 1.410\% & 1.158 & 0.554\% \\
9,801  & 1.766 & 0.035\% & 14.486 & 0.287\% & 5.810 & 0.115\% \\
89,401  & 1.524 & 0.003\% & 2.432 & 0.005\% & 2.292 & 0.005\% \\
249,001  & 2.212 & 0.002\% & 3.670 & 0.003\% & 3.324 & 0.003\% \\ \bottomrule
\end{tabular}%
% }
\caption{Mean sums of absolute differences for \gls{pe} ending values between the \gls{ls} and asynchronous variants in a simulation, with different sending delay lengths, on a computer with a CPU with 24/48 physical/logical cores}
\label{tab:nmp:diffs48cores}
\end{table}

Tables \ref{tab:nmp:diffs8cores}-\ref{tab:nmp:diffs48cores} show the results of computing the absolute differences, both as their sums, and those sums as a percentage of the grid-wide sum of final \gls{pe} values from the \gls{gs}/\gls{ls} processing.  The latter metric conveys the overall magnitude of difference in results.  The results demonstrate that the difference is less than 2\% in all cases, and usually less than one-hundredth of 1\% on larger grids --- supporting Conjecture \ref{conj:nmp:1}.  This also appears to provide some evidence for Conjecture \ref{conj:nmp:2}.  It seems unlikely that a difference of under 1\% will have much impact on the `quality' of the final results.

\subsection{Progressive completion of proxels}
Conceivably, in some problem domains, valuable information can be gleaned from a partially completed \gls{nmp} process.  Given that each \gls{pe} runs independently, they will not necessarily all finish simultaneously --- particularly with the \gls{ls} and asynchronous variants.

The \gls{ls} and asynchronous variants provide each \gls{pe} with a greater ability to act as and when appropriate to them, and as shown in \autoref{sec:nmp:timingexp} also tend to complete the entire process more quickly.  These factors combined suggest that some \glspl{pe} will complete their respective duties long before the grid as a whole has reached the end.  If so, then problems in the aforementioned domains perhaps do not need to wait until the entire grid has finished before using the computed information or prompting some action.

To investigate the likelihood of a sizeable proportion of the \glspl{pe} finishing much earlier, the \gls{nmp} program used in the earlier sections was modified so that every \gls{pe} stores the current time elapsed since the start of measurement when it finishes processing.  These times are then written out at the end of all processing along with the other measurements collected.  The measurements were then processed to identify the minimum, mean, and maximum completion times for \glspl{pe} on the grid.  I.e., the number of elapsed milliseconds since the start of the \gls{nmp} process until the first \gls{pe} recorded its completion time; the arithmetic mean of the completion times across the entire grid; and, the elapsed milliseconds until the last \gls{pe} to finish recorded its completion time.

\begin{table}
\centering
\begin{tabular}{@{}rcrrrrr@{}}
\toprule
\multicolumn{1}{l}{\begin{tabular}[c]{@{}l@{}}\# of \\ \glspl{pe}\end{tabular}} &
  Variant &
  Min &
  Mean &
  Max &
  \begin{tabular}[c]{@{}r@{}}Diff min \\ max \%\end{tabular} &
  \begin{tabular}[c]{@{}r@{}}Diff mean \\ max \%\end{tabular} \\ \midrule
110   & Global & 1,267   & 1,274   & 1,284   & 1.324  & 0.779  \\
110   & Local  & 320     & 800     & 1,271   & 74.823 & 37.057 \\
110   & Async  & 365     & 720     & 1,080   & 66.204 & 33.333 \\
2,550  & Global & 27,247  & 27,528  & 27,861  & 2.204  & 1.195  \\
2,550  & Local  & 21,710  & 22,939  & 24,051  & 9.733  & 4.624  \\
2,550  & Async  & 21,962  & 22,743  & 23,518  & 6.616  & 3.295  \\
10,100 & Global & 103,565 & 104,825 & 106,199 & 2.480  & 1.294  \\
10,100 & Local  & 98,746  & 101,900 & 103,782 & 4.852  & 1.813  \\
10,100 & Async  & 98,347  & 100,522 & 100,670 & 2.308  & 0.147  \\ \bottomrule
\end{tabular}
\caption{Minimum, average, and maximum processing times in milliseconds for \glspl{pe} for varying grid sizes, and the difference between the minimum \& maximum times plus the mean \& maximum times expressed as a percentage of the maximum time, running on an 8 core CPU.}
\label{tab:nmp:progressive8cores}
\end{table}

\begin{table}
\centering
\begin{tabular}{@{}rcrrrrr@{}}
\toprule
\multicolumn{1}{l}{\begin{tabular}[c]{@{}l@{}}\# of \\ \glspl{pe}\end{tabular}} &
  Variant &
  Min &
  Mean &
  Max &
  \begin{tabular}[c]{@{}r@{}}Diff min \\ max \%\end{tabular} &
  \begin{tabular}[c]{@{}r@{}}Diff mean \\ max \%\end{tabular} \\ \midrule
110   & Global & 1,290  & 1,296  & 1,304  & 1.074  & 0.613  \\
110   & Local  & 471    & 848    & 1,267  & 62.826 & 33.070 \\
110   & Async  & 352    & 725    & 1,094  & 67.824 & 33.729 \\
2,550  & Global & 22,604 & 22,797 & 22,990 & 1.679  & 0.839  \\
2,550  & Local  & 16,159 & 17,277 & 18,469 & 12.507 & 6.454  \\
2,550  & Async  & 16,502 & 17,235 & 18,260 & 9.628  & 5.613  \\
10,100 & Global & 76,548 & 77,319 & 78,105 & 1.993  & 1.006  \\
10,100 & Local  & 71,256 & 73,009 & 74,145 & 3.896  & 1.532  \\
10,100 & Async  & 71,148 & 72,484 & 72,701 & 2.136  & 0.298  \\ \bottomrule
\end{tabular}
\caption{Minimum, average, and maximum processing times in milliseconds for \glspl{pe} for varying grid sizes, and the difference between the minimum \& maximum times plus the mean \& maximum times expressed as a percentage of the maximum time, running on a 12 core CPU.}
\label{tab:nmp:progressive12cores}
\end{table}

\begin{table}
\centering
\begin{tabular}{@{}rcrrrrr@{}}
\toprule
\multicolumn{1}{l}{\begin{tabular}[c]{@{}l@{}}\# of \\ \glspl{pe}\end{tabular}} &
  Variant &
  Min &
  Mean &
  Max &
  \begin{tabular}[c]{@{}r@{}}Diff min \\ max \%\end{tabular} &
  \begin{tabular}[c]{@{}r@{}}Diff mean \\ max \%\end{tabular} \\ \midrule
110   & Global & 1,474  & 1,477  & 1,482  & 0.540  & 0.337  \\
110   & Local  & 451    & 915    & 1,364  & 66.935 & 32.918 \\
110   & Async  & 399    & 792    & 1,124  & 64.502 & 29.537 \\
2,550  & Global & 12,109 & 12,171 & 12,235 & 1.030  & 0.523  \\
2,550  & Local  & 6,056  & 7,103  & 8,838  & 31.478 & 19.631 \\
2,550  & Async  & 6,106  & 6,972  & 8,592  & 28.934 & 18.855 \\
10,100 & Global & 36,665 & 37,425 & 38,609 & 5.035  & 3.067  \\
10,100 & Local  & 29,791 & 31,363 & 33,348 & 10.666 & 5.952  \\
10,100 & Async  & 30,299 & 31,018 & 31,534 & 3.916  & 1.636  \\ \bottomrule
\end{tabular}
\caption{Minimum, average, and maximum processing times in milliseconds for \glspl{pe} for varying grid sizes, and the difference between the minimum \& maximum times plus the mean \& maximum times expressed as a percentage of the maximum time, running on a 48 core CPU.}
\label{tab:nmp:progressive48cores}
\end{table}

Tables \ref{tab:nmp:progressive8cores}-\ref{tab:nmp:progressive48cores} detail the recorded results from the progressive completion measurements across grids of assorted sizes.  Here, grids of 11×10, 51×50 and 101×100 \glspl{pe} were used.  As above, all measurements and are presented in milliseconds.  The left-most column in each table is the number of \glspl{pe} in the grid from which the timings were recorded.  The second left-most column lists which variant of the \gls{nmp} system was used for that entry.  The middle three columns record the total processing times for the fastest, average, and slowest \glspl{pe}, respectively.  The right-most two columns express the difference between the times for the fastest and slowest, or the average and slowest, \glspl{pe} respectively.  In these latter two, the differences are expressed as a percentage of the slowest \gls{pe}'s running time.

As a proportion of the maximum running time, the largest difference between the minimum and maximum running times is seen on the 110 \gls{pe} grid with the 8 core CPU using the \gls{ls} approach, where there is still roughly 75\% of the total time to go when the fastest \gls{pe} finishes.  The largest gap from mean to maximum running times is also observed in the same experiment.  The proportionately smallest difference between minimum and maximum is seen on the 110 \gls{pe} grid with the 48 core CPU using the \gls{gs} approach, where there is just a 0.5\% time difference.  The smallest difference from the mean to the maximum is found on the 10,100 \gls{pe} grid with the 8 core CPU using the asynchronous approach, where there is a 0.14\% difference.  It appears that on smaller grids, the time difference from the fastest to the slowest \glspl{pe} completing can be significant, but on larger grids, it is usually a relatively small quantity compared to the total running time.

In absolute terms, the largest difference between the minimum and maximum running times is observed on the 10,100 \gls{pe} grid with the 8 core CPU, using the \gls{ls} approach, with a gap of 5.036 seconds.  At 1.985 seconds, the largest gap from mean to maximum is also seen using the \gls{ls} variant on the 10,100 \gls{pe} grid, but running on the 48 core CPU.  The smallest minimum to maximum gap is eight milliseconds, observed on the 110 \gls{pe} grid with the 48 core CPU and using the \gls{gs} variant.  The shortest mean to maximum gap is seen in the same experiment, at five milliseconds.

\begin{figure}
    \centering
    \begin{tikzpicture}
    \begin{groupplot}[
            group style={
                group size=3 by 1,
                xlabels at=edge bottom,
                ylabels at=edge left,
            },
        	symbolic x coords={Min,Mean,Max},
            ylabel=Milliseconds,
            xtick=data,
            small,
            height=0.27\textheight,
            width=0.37\textwidth,
            % legend columns=-1,
            % legend entries={\Gls{gs}, \Gls{ls}, Asynchronous},
            % legend to name=progressiveleg,
    ]
    \nextgroupplot
    \addplot % global-sync
    	coordinates {(Min, 1267) (Mean, 1274) (Max, 1284)};
    \addplot % local-sync
    	coordinates {(Min, 320) (Mean, 800) (Max, 1271)};
	\addplot % async
    	coordinates {(Min, 365) (Mean, 720) (Max, 1080)};
        
    \nextgroupplot
    \addplot % global-sync
    	coordinates {(Min, 27247) (Mean, 27528) (Max, 27861)};
    \addplot % local-sync
    	coordinates {(Min, 21710) (Mean, 22939) (Max, 24051)};
	\addplot % async
    	coordinates {(Min, 21962) (Mean, 22743) (Max, 23518)};
    	
	\nextgroupplot
	\addplot % global-sync
        	coordinates {(Min, 103565) (Mean, 104825) (Max, 106199)};
    \addplot % local-sync
    	coordinates {(Min, 98746) (Mean, 101900) (Max, 103782)};
	\addplot % async
    	coordinates {(Min, 98347) (Mean, 100522) (Max, 100670)};
    	
    \end{groupplot}
    \end{tikzpicture}
    % \ref{timingsleg}
    \caption{Charts comparing the minimum, mean, and maximum termination times for \glspl{pe} executing on a CPU with 8 cores, for grids with (from left) 110, 2,550 and 10,100 \glspl{pe}, respectively.  See also \autoref{tab:nmp:progressive8cores}}
    \label{fig:nmp:progressive8cores}
\end{figure}

\begin{figure}
    \centering
    \begin{tikzpicture}
    \begin{groupplot}[
            group style={
                group size=3 by 1,
                xlabels at=edge bottom,
                ylabels at=edge left,
            },
        	symbolic x coords={Min,Mean,Max},
            ylabel=Milliseconds,
            xtick=data,
            small,
            height=0.27\textheight,
            width=0.37\textwidth,
    ]
    \nextgroupplot
    \addplot % global-sync
    	coordinates {(Min, 1290) (Mean, 1296) (Max, 1304)};
    \addplot % local-sync
    	coordinates {(Min, 471) (Mean, 848) (Max, 1267)};
	\addplot % async
    	coordinates {(Min, 352) (Mean, 725) (Max, 1094)};
        
    \nextgroupplot
    \addplot % global-sync
    	coordinates {(Min, 22604) (Mean, 22797) (Max, 22990)};
    \addplot % local-sync
    	coordinates {(Min, 16159) (Mean, 17277) (Max, 18469)};
	\addplot % async
    	coordinates {(Min, 16502) (Mean, 17235) (Max, 18260)};
    	
	\nextgroupplot
	\addplot % global-sync
        	coordinates {(Min, 76548) (Mean, 77319) (Max, 78105)};
    \addplot % local-sync
    	coordinates {(Min, 71256) (Mean, 73009) (Max, 74145)};
	\addplot % async
    	coordinates {(Min, 71148) (Mean, 72484) (Max, 72701)};
	
    \end{groupplot}
    \end{tikzpicture}
    % \ref{timingsleg}
    \caption{Charts comparing the minimum, mean, and maximum termination times for \glspl{pe} executing on a CPU with 12 cores, for grids with (from left) 110, 2,550 and 10,100 \glspl{pe}, respectively.  See also \autoref{tab:nmp:progressive12cores}}
    \label{fig:nmp:progressivecharts12cores}
\end{figure}

\begin{figure}
    \centering
    \begin{tikzpicture}
    \begin{groupplot}[
            group style={
                group size=3 by 1,
                xlabels at=edge bottom,
                ylabels at=edge left,
                % width=\textwidth
            },
        	symbolic x coords={Min,Mean,Max},
            ylabel=Milliseconds,
            xtick=data,
            small,
            height=0.27\textheight,
            width=0.37\textwidth,
    ]
    \nextgroupplot
    \addplot % global-sync
    	coordinates {(Min, 1474) (Mean, 1477) (Max, 1482)};
    \addplot % local-sync
    	coordinates {(Min, 451) (Mean, 915) (Max, 1364)};
	\addplot % async
    	coordinates {(Min, 399) (Mean, 792) (Max, 1124)};
        
    \nextgroupplot
    \addplot % global-sync
    	coordinates {(Min, 12109) (Mean, 12171) (Max, 12235)};
    \addplot % local-sync
    	coordinates {(Min, 6056) (Mean, 7103) (Max, 8838)};
	\addplot % async
    	coordinates {(Min, 6106) (Mean, 6972) (Max, 8592)};
    	
	\nextgroupplot
	\addplot % global-sync
        	coordinates {(Min, 36665) (Mean, 37425) (Max, 38609)};
    \addplot % local-sync
    	coordinates {(Min, 29791) (Mean, 31363) (Max, 33348)};
	\addplot % async
    	coordinates {(Min, 30299) (Mean, 31018) (Max, 31534)};
	
    \end{groupplot}
    \end{tikzpicture}
    % \ref{timingsleg}
    \caption{Charts comparing the minimum, mean, and maximum termination times for \glspl{pe} executing on a CPU with 48 cores, for grids with (from left) 110, 2,550 and 10,100 \glspl{pe}, respectively.  See also \autoref{tab:nmp:progressive48cores}}
    \label{fig:nmp:progressivecharts48cores}
\end{figure}

Visual comparisons of the information listed in Tables \ref{tab:nmp:progressive8cores}-\ref{tab:nmp:progressive48cores} are shown in Figures \ref{fig:nmp:progressive8cores}-\ref{fig:nmp:progressivecharts48cores}.  Each figure shows three line charts, which compare the timings (from left) for the grids with 110, 2,550, and 10,100 \glspl{pe} respectively.  The charts highlight that the size of the gap between the minimum and maximum times for the \gls{gs} approach tend to be much smaller, as clear from the relatively flat slopes of the lines.  For the asynchronous case, while the minimum is usually notably lower than the maximum, the mean often appears to be only slightly earlier than the maximum, which suggests that the latter half of the \glspl{pe} come to a stop quickly.  In the case of the \gls{ls} approach, however, there can be a noticeable difference between all three measurements.

Considering that, in many cases, the \gls{ls} and asynchronous approaches complete entirely before the fastest \gls{pe} finishes in the \gls{gs} process, a significant benefit is already available from using one of the other two, without considering progressive completion.

Based on these results and those of \autoref{sec:nmp:convergence}, it appears that, in general, the asynchronous approach should be taken as the `default' when an explicit message passing implementation for \gls{nmp} is used.  If the problem domain at hand is one where precisely the same final result as the \gls{gs} variant is essential, \emph{and} an acceptable result can be deduced from only approximately half of the \glspl{pe} completing, then the \gls{ls} variant should be used.

\subsection{Experimental limitations}
The results above indicate trends that might be drawn, but there are limitations to the data.  These limitations primarily stem from a lack of diversity in the testing.  The experiments were conducted using only one programming language and runtime, C\# on .NET (specifically version 5.0.301 of .NET).  All processors used were Intel x86\_64 processors.  All tests were performed on computers running either Windows 10 or Windows Server 2016.  We conjecture that similar results will be obtained on other platforms, assuming that due consideration is paid to the proper granularity of \gls{pe} tasks to processors.  Indeed, we predict that more noteworthy results will be seen on platforms that use lightweight tasks.

Arguably, too few iterations of each test were performed to consider the results statistically valid, although we note that, usually, the difference between the minimum and maximum running times for each test was at most a few percent.  All timing experiments were performed with a ratio of one \texttt{Task} to one \gls{pe}.  This was a direct correspondence between the \gls{cps} models and the practical implementation, but may be sub-optimal for resource allocation.  A thorough exploration of the right granularity of \glspl{pe} for available processing resources could be made in the future.

All experiments were based on \gls{fne} lattices.  The theorems and lemmas described in \autoref{sec:nmp:msgprops} hold for lattices of any size neighbourhood, but the performance of \gls{nmp} implementations may (or may not) vary according to the number of neighbours ordinary \glspl{pe} have.  The effect is uncertain at this point.  On the one hand, a higher number of neighbours could force the synchronous variants to become more synchronised relative to the asynchronous variant, which would appear to favour the latter.  On the other hand, the overheads of stepping through the messaging loop more often might outweigh the benefits of the asynchronicity.  The balance between them likely is implementation-dependent.