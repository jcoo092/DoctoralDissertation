\section{Summary}
After briefly summarising core aspects of \gls{cps}, this \namecref{chap:nmp} presented three variants of a generic system for \gls{nmp} computation on a lattice in \gls{cps}:
\begin{inparaenum}[(i)]
\item a \gls{gs} one in which every top-level cell, representing a \gls{pe}, performs the same actions simultaneously;
\item an asynchronous one where different top-level cells \emph{may} be in different states and perform different actions simultaneously and may be at a different generation count to their neighbours; and
\item a \gls{ls} one, partway between the other two, in which \glspl{pe} evolve separately, but synchronise sufficiently to keep to the same generation count across the system.
\end{inparaenum}

A small example was then provided, aiming to clarify the operation and overall procession of the rules.  Next, \cref{sec:nmp:analysis} analysed the asynchronous system.  The \namecref{sec:nmp:analysis} proved that in both the synchronous and asynchronous versions, every \gls{pe} sends exactly one message per neighbour, per generation, and halts once it has reached its maximum number of generations.  The main difference between the two systems is that in preparing the outgoing message for neighbour \(k\) and generation \(g + 1\), some received input messages may already be from a later generation (greater than \(g\)).  No message used will ever be from a generation less than \(g\), however.

The asynchronous system has a relatively low level of rule and symbol complexity for a \gls{cps} implementation, but all the systems in this \namecref{chap:nmp} are only a partial view:  for (relative) brevity, simplicity, and genericity, an oracle was used to abstract over the rules for \gls{pe}-internal computations, the nature of which are specific to a problem's application domain.  The variants are essentially a high-level abstraction of various \gls{nmp}-based algorithms employing communication.  While there can be significant differences in the nature of the computations performed within each \gls{pe}, they all have fundamentally the same structure:  Computing updates over some data within a given \gls{pe}, and then exchanging those data with other \glspl{pe}.

Multiple experiments were performed to investigate various aspects of the \gls{nmp} variants.  After validating \cref{theorem:nmp:-4} with a working program, overall completion times for each variant, across multiple grid sizes and different CPUs with different core counts, were measured.  The results showed that in nearly all cases, the asynchronous variant was fastest, although, on smaller grid sizes and fewer cores, the \gls{ls} variant was as fast.

Final \gls{pe} values from the same starting conditions were compared between the \gls{gs} \& \gls{ls} variants, and the \gls{ls} \& asynchronous variants.  The results showed that the \gls{gs} \& \gls{ls} variants always produce identical final results, while the asynchronous variant's final results are usually close to those of the other two.  The time to completion for the fastest, average and slowest \glspl{pe} in the grid were determined and analysed.  All three times tended to be quite close for the \gls{gs} approach.  The time to completion for the fastest and average \glspl{pe} tended to be similar between the \gls{ls} and asynchronous approaches, but usually, the slowest \gls{pe} finished notably earlier with the asynchronous one.

% \subsection{Future Work}
% The next step in this work is to adapt the system to the purpose of Belief Propagation Stereo Matching, using \gls{nmp} precepts.  This will be presented in Part Two.  Furthermore, work has begun on implementing a close approximation of the asynchronous system as a framework in a standard programming languages, to explore the effectiveness of this approach in modern computer systems.  We plan to implement Belief Propagation Stereo Matching (see e.g. \cite{Blake2011,Felzenszwalb2011,JianSun2003}) atop this as a proof-of-concept.

% One aspect the systems presented above lack is that both the size and shape of the grid involved, as well as the communication topology between neighbours, are permanently fixed at the time of system initialisation.  In most cases this is unneeded, but the greater flexibility could be of use when implementing certain algorithms.

% Furthermore, at present it is implicitly assumed that every \gls{pe} remains active throughout the entirety of the system's evolution until it has sent and received all of its scheduled messages.  Within the context of \gls{cps} this is largely irrelevant, but permitting \glspl{pe} to deactivate at appropriate points could save processing power in other circumstances with bounded parallelism.  Complicating this is ensuring that those \glspl{pe} which do remain active can continue messaging as needed despite one or more neighbours deactivating.

% We also have yet to examine the systems with respect to communication complexity measures such as those found in \cite{Juayong2020}.  The precise results presented there are not directly applicable to this work, given the use of different P~systems models, but the underlying concepts appear directly relevant.