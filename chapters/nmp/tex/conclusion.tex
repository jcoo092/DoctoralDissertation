\section{Summary}
This \namecref{chap:nmp} presented three variants of a generic system for \gls{nmp} computation on a lattice in \gls{cps}:
\begin{inparaenum}[(i)]
\item a \gls{gs} one in which every \gls{tlc}, representing a \gls{pe}, performs the same actions simultaneously;
\item an asynchronous one where different \glspl{tlc} \emph{may} be in different states, perform different actions simultaneously and may be at a different generation count to their neighbours; and
\item a \gls{ls} one, partway between the other two, in which \glspl{pe} evolve separately, but synchronise sufficiently to keep to the same generation count across the system.
\end{inparaenum}

A small example for the asynchronous variant was then provided, aiming to clarify the operation and overall procession of the rules.  Next, \cref{sec:nmp:analysis} analysed the asynchronous system proved that in both the synchronous and asynchronous versions, every \gls{pe} sends exactly one message per neighbour, per generation, and halts once it has reached its maximum number of generations.  The main difference between the two is that in preparing the outgoing message for neighbour \(k\) and generation \(g + 1\) under the asynchronous variant, some received input messages may already be from a later generation (greater than \(g\)).  No message used will ever be from a generation less than \(g\), however.

The asynchronous system has a relatively low level of rule and symbol complexity for a \gls{cps} implementation, but all the systems in this \namecref{chap:nmp} are only a partial view:  oracles were used to abstract over the rules for \gls{pe}-internal computations, the nature of which are specific to a problem's application domain.  The variants are essentially a high-level abstraction of various \gls{nmp}-based algorithms employing communication.  While there can be significant differences in the nature of the computations performed within each \gls{pe}, they all have fundamentally the same structure:  computing updates over some data within a given \gls{pe}, and then exchanging those updated data with other \glspl{pe}.

Multiple experiments were performed to investigate various aspects of the \gls{nmp} variants.  After validating \cref{theorem:nmp:-4} with a working program, overall completion times for each variant, across multiple grid sizes and different CPUs with different core counts, were measured.  The results showed that in nearly all cases, the asynchronous variant was fastest.  Although, on smaller grid sizes and fewer cores, the \gls{ls} variant was as fast.

Final \gls{pe} values from the same starting conditions were compared between the \gls{gs} \& \gls{ls} variants, and the \gls{ls} \& asynchronous variants.  The results showed that the \gls{gs} \& \gls{ls} variants always produce identical final results, while the asynchronous variant's final results are usually close to those of the other two.  The time to completion for the fastest, average and slowest \glspl{pe} in the grid were determined and analysed.  All three times tended to be quite close for the \gls{gs} approach.  The time to completion for the fastest and average \glspl{pe} tended to be similar between the \gls{ls} and asynchronous approaches, but usually, the slowest \gls{pe} finished notably earlier with the asynchronous one.