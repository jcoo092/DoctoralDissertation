\section{Introduction}
This \namecref{chap:nmp} presents a framework for passing messages between discrete points on a finite lattice, where each point updates its internal data (and thus the messages it sends) based on messages received.  Each point is uniquely connected to other points and so has its own \emph{neighbourhood}, being the specific other points it communicates with.  A key aspect of this \emph{\gls{nmp}} computation is that the messages to a given neighbour depend upon the messages received previously from \emph{all} neighbours, \emph{except} the neighbour to which the current message is sent.    This necessarily means that in \gls{nmp} each grid location must have at least two neighbours; a single neighbour cannot form a meaningful neighbourhood.  This \namecref{chap:nmp} focuses on the \emph{square grid lattice}
but the principles of \gls{nmp} apply equally to all lattices.%focuses on the \emph{square grid lattice} but \gls{nmp} applies equally to lattices of any shape, dimension, or connectivity.

In this \namecref{chap:nmp}, the individual computational units in the lattice are termed \emph{\glspl{pe}}.\footnote{The short form ``\gls{pe}'' is derived from “processing element'' in the same way that “pixel'' is derived from “picture element''.}  These are the logical base units for computation for \gls{nmp} and are represented by \gls{cps} \glspl{tlc}.  A visual example of the \gls{nmp} process for the \emph{\gls{fne}} on the square lattice is shown in \cref{fig:nmp:gridmessaging}.  A \gls{pe} in the grid received messages from its neighbours to the left, right, and bottom at generation \(t - 1\), and used the data from those messages to prepare its new message to be sent to the neighbour above at iteration \(t\).  The same is performed for every other neighbour too.

\begin{figure}
    \centering
    \includegraphics[keepaspectratio,width=1.0\linewidth]{chapters/nmp/images/bp_diagram_recoloured_iotacentre.pdf}
    \caption[Diagram of the central concept of \gls{fne} \glsxtrlong{nmp} on a grid]{Diagram of the central concept of \gls{fne} \gls{nmp} on a grid \cite{lbpmpsmpic}.  Each circle represents a \gls{pe}, labelled following the pattern of \cref{fig:nmp:iota_proxels_environment_oracle}.  The central \gls{pe} \(\iota\) needs to compute a new message to send at time \(t\) to \gls{pe} \(1\) above it, so it performs a computation to combine the results of the messages received from the other three neighbouring cells (\(2\), \(3\) \& \(4\)) at time \(t - 1\) (indicated by the thin and fat arrows).  The same is applied to every other neighbour, too.}
    \label{fig:nmp:gridmessaging}
\end{figure}

Conceptually, there are two distinct aspects of the \gls{nmp} process:
\begin{inparaenum}
\item the message passing itself, which occurs externally to \glspl{pe} over channels connected to other \glspl{pe}; and,
\item the data/message updates, which occur internally.
\end{inparaenum}
The focus of this \namecref{chap:nmp} is the external side, and communication with an oracle stands in for the internal computation, which is largely orthogonal to the inter-\gls{pe} communication.

There are (at least) two separate ways to model the \glspl{pe}' communication:  a \emph{\gls{gs}} system-wide view, where the states and steps taken occur across the system as a whole and every \gls{pe} carries out its operations simultaneously according to the same rules.  Or, a per-\gls{pe} view, where each \gls{pe} has its own state and advances \emph{asynchronously}\footnote{Recall from \vref{sec:back:syncasync} that the idea of asynchronicity used here follows the traditional concept from distributed computing, and so is \emph{different} to that sometimes used in other \gls{ps}.  Briefly, in the asynchronous model of distributed algorithms, external messages may take any arbitrary time (in \(\mathbb{R}\)) to reach their destinations \cite{Balanescu2011,Nicolescu2014,Lynch1996}, though internal updates are typically assumed to occur (near-)instantaneously.} without regard to others' objects, states, and \glspl{ruleset}.  This \namecref{chap:nmp} explores both and finds in the process that an intermediate third, \emph{\gls{ls}}, model arises naturally as a hybrid of the other two.

Modelling and implementation of \gls{nmp} proves surprisingly challenging.  The \gls{gs} form (\cref{sec:nmp:globalsync}) is reasonably simple.  It requires only nine rules, most of which have exactly one term on both the left-hand- and right-hand-sides, and none of which use \glspl{promoter} or \glspl{inhibitor} (see \cref{chap:cpsystems} for more on these concepts).

The challenge arises with the asynchronous version.  The possibility of messages from the same generation arriving at different times necessitates bookkeeping.
\Glspl{pe} must track from which neighbours messages have been received for a given generation, to decide to which neighbours the \gls{pe} is ready to send a new message.
This leads to inescapable complexity and the undesirable possibility of deadlock through unsuitable design.

\Gls{nmp} shares similarities and overlaps with areas such as Cellular Automata, Gossip Protocols and Consensus Algorithms (see, \eg{}, \cite{Deserable2012,Hollander2015}).  \Gls{nmp} is distinguished from these other areas in a few ways, however.  Firstly, other approaches do not usually impose the ``\(n - 1\) neighbours'' condition of message updates as found in \gls{nmp}.  Secondly, in other areas messaging tends to be only a part of the process, whereas in \gls{nmp} it is the heart of it.  Internal computation is only performed to aggregate/average the messages received from neighbours to prepare new messages.  Thirdly, neither a global nor local consensus is sought in \gls{nmp} --- the closest equivalent would be the (tautological) concept of individual consensus per \gls{pe}.  Instead, the result of each \gls{pe} comprises one distinct part of the final result.  Lastly, often in these other areas, the neighbours are selected at random rather than being a neighbourhood arising naturally from the structure of the modelled problem.  Generalisation across these fields may be possible but is not pursued further.

This \namecref{chap:nmp} begins by describing a straightforward \gls{gs} \gls{nmp} system.  Then, it provides an asynchronous \gls{pe}-specific version of the same, followed by an adaptation of the asynchronous system to a \gls{ls} system. Next is a short example of a potential evolution of the asynchronous system to clarify its operation.  Lastly, this \namecref{chap:nmp} analyses the asynchronous system and reports the results of comparative computer experiments for all three systems.  The analysis proves that the asynchronous system sends precisely the same number of messages as the \gls{gs} system (one per generation per neighbour) but that the data used to compute new messages may vary slightly depending on the ordering of messages received.  Furthermore, the empirical results verify the operation of the asynchronous system and support the hypothesis that the \gls{ls} and asynchronous systems are faster than an equivalent \gls{gs} version.

\subsection{Belief Propagation}

This \namecref{chap:nmp} was originally motivated by attempts to model \emph{\gls{lbp}} for \gls{sm} (see \eg{} \cite{Blake2011,Felzenszwalb2011,Sun2003}) in \gls{cps}.  \emph{\Gls{bp}} was originally introduced to solve inference problems in AI on graphs \cite{Pearl1982} by treating the nodes as communicating objects, which passed messages backwards and forwards, updating their outgoing messages based on the incoming ones, to solve the problem.  The original formulation relied upon the graphs having a tree structure, however.  Each node would receive a message from its parent, pass that to its children, receive new messages back from the children and pass its own newly computed message back to its parent based on those received from the children.

In the case of \gls{sm}, the output is a (typically rectangular) grid representing an output image.  This means that the computation, sited on this grid, is inherently non-tree-structured.  Instead, each output node is connected to its neighbours -- usually in a \gls{fne} -- which means that messages travel in loops on the grid.  To deal with this situation, \gls{bp} was adapted to \gls{lbp}, which was first applied to \gls{sm} by \citeauthor{Sun2003} \cite{Sun2003}.  The algorithm was then significantly improved upon by \citeauthor{Felzenszwalb2006} \cite{Felzenszwalb2006}.

\Gls{nmp}, as described in this \namecref{chap:nmp}, can simulate \gls{lbp} effectively, and \cref{sec:nmp:example,sec:nmp:analysis,sec:nmp:experiments} focus on the \gls{fne}.  The main adaptation required is to replace the oracle with appropriate computations for \gls{lbp} \gls{sm}.  \Gls{nmp} can be generalised further, however, to perform other computations or use other communication arrangements besides the \gls{fne}.  While this work was originally inspired by \gls{lbp}, it is a generic framework for \emph{any} computations that may be modelled with communication on a lattice, when the outgoing message to one neighbour depends on the messages received from all other neighbours.