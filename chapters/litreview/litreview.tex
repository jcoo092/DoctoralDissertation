\chapter{Related Work}
This review of related past work focuses on the three main areas of Computer Science that are relevant to this dissertation:  Stereo Matching, Formal Models of Computation (especially \gls{ps}), and \gls{cml}.  In none of these cases does the review come close to comprehensively covering the entire span of the given area.  It merely tries to cover as much of the relevant recent and classic work as possible, while giving an extremely brief introduction to these areas.  The interested reader who is unfamiliar with any of these topics is strongly advised to refer to the references cited.

\section{\glsentrylong{mpbsm}}

% \subsection{Preliminaries}
% What do I actually need to put in here?

% \subsubsection{Stereo Matching}

\subsection{\glsentrylong{sm}}

\subsubsection{Local vs Global}
\cite{Scharstein2002}

\subsubsection{Markov Random Fields \& Bayesian Statistics}
\cite{Kolmogorov2015,Blake2011}

Geman \& Geman \cite{Geman1984} showed, however, that Markov Random Fields are equivalent to Gibbs Distributions and that the two could be applied usefully to image tasks \cite{Gimelfarb1999}.

Frequently, in global methods the function used for the data cost is quite simplistic.  Most common is the use of simple absolute difference between the intensities of the pixels compared.  Other popular methods include \gls{sad} and \gls{ssd}, adaptive window methods \cite{Yoon2005,Yoon2006}, and Birchfield \& Tomasi's Pixel Dissimilarity Measure \cite{Birchfield1998}.

In general, most Markov Random Field approaches to \gls{sm} tend to use a truncated linear function to estimate the discontinuity/smoothness cost.  Such a function typically takes a form such as \[ E_{discontinuity} = \alpha \times min(| d_p - d_q |, \beta) \] where, for the purposes of this equation, \(E_{discontinuity}\) represents the total estimated cost of the assignment; \(\alpha\) is a scaling coefficient that may or may not be used; \(\beta\) is a constant that provides the upper limit to the cost estimate; and \(d_p\) and \(d_q\) are the proposed labels of the current pixel and its neighbour currently under consideration.  While a simple absolute difference function is perhaps the most common applied to the labels, it is important to note that it is not the only one that could be used.  

For example, Ha and Jeong \cite{Ha2016} use a two-step Potts model, with different penalties for a difference of 1 compared to a difference of 2 or greater. Conversely, Tan \textit{et al.} \cite{Tan2017} comment that a typical Potts function can be viewed as a special case of the absolute difference truncated linear function, where the truncation value (\(\beta\) in the equation above) is 1, while the coefficient is the value of the Potts penalty parameter.

The choice of the truncated linear function is motivated by the assumption that most surfaces in images either are planar, or smoothly vary in disparities, and thus larger jumps should be penalised more heavily, but very large jumps are almost certainly indicative of an object boundary where a large difference in disparities is warranted.  Therefore, at a certain point, the penalty to assign significantly different values should stop growing, so as not to reduce the likelihood of correctly assigning large differences in disparities at object edges.

\subsection{\glsentrylong{bp}}
\gls{bp} was introduced by Pearl \cite{Pearl1982} for use with inference engines, in the context of Bayesian Statistics and Gibbs Distributions.  \gls{bp} was first applied to \gls{sm} in \cite{Sun2003} where it demonstrated excellent performance compared to many contemporary matching algorithms, but the `breakthrough' paper was arguably \cite{Felzenszwalb2006}, where a near-real time implementation was presented which still had extremely good results.

Yang \textit{et al.} \cite{Yang2006a} built upon hierarchical \gls{bp}, adding in extra steps before and after the \gls{bp} process.  They combine information derived from using the mean shift algorithm \cite{Comaniciu2002}; a colour-weighted correlation method based on Yoon \& Kweon's \cite{Yoon2006} applied to both the left and right images; a left-right consistency check to detect occluded pixels; a plane-fitting process based on Tao \& Sawhney's \cite{Tao2000}; as well \gls{bp} itself.  While combining these various techniques leads to a highly-accurate disparity map,\footnote{This algorithm achieved the top ranking on Middlebury when it was first introduced.} it is \emph{extremely} slow.

Typical \gls{bp} uses the four-connected neighbourhood to define the neighbours of each given node in the grid.  This means that each node passes messages to and from it's immediate neighbours up, down and to the left and right of it in the grid.  Other neighbourhood arrangements are possible though, depending on the underlying model one wants to use.  For example, Tan \textit{et al.} \cite{Tan2017} describe an approach to \gls{bp} where every pixel is considered to be a neighbour of every other pixel.  Messages are weighted according to the distance across the grid between the neighbours, with nearer neighbours having a greater impact upon a pixel's final beliefs.  The major advantage of this approach is that it almost eliminates the need for repeated iterations of message passing.

Ha and Jeong \cite{Ha2016} suggested a different approach for scheduling the messages.  Instead of each pixel repeatedly exchanging messages with its neighbours until a reasonable amount of the grid has been spanned, they start in one of the corners in the image, and sequentially pass messages along two directions until reaching the other corner, repeating this process once for each corner.  The great advantage of this is that in principle one only needs to perform message exchanges in each direction once.  Their implementation still required roughly \SI{3.5}{\second} to complete, however, without returning a significantly more accurate disparity map.\footnote{The authors claimed that their method was \numrange{300}{600} times faster than `standard` \gls{bp}, but they did not specify their stopping condition.  Based on their reported results it appears that they used well over 300 iterations on an image -- many more than would be reasonable for image sizes likely to be targeted for real-time \gls{sm}.}

Balossino \textit{et al.} \cite{Balossino2007} suggested an alternative formulation to the traditional grid of Loopy \gls{bp}.  Instead, they built a forest of trees, each of which was rooted at the given pixel under consideration, and which has a handful of neighbouring pixels as children.  The attraction of this approach is that it restores the properties of optimality and convergence described in \cite{Pearl1982} for one round of messages up and down each tree.  This advantage is tempered, however, by the necessity of combining results from different trees.  The final accuracy appeared to be worse than with \cite{Felzenszwalb2006}, and there was no reporting of the running time, though it seems unlikely that this approach was fast.

It should be noted that \gls{bp} is \emph{not} regarded as the current top-performing \gls{sm} algorithm.  In 2013 Tippets \textit{et al.} found that, of algorithms implemented on the CPU, blah was the fastest `accurate' method, while blah provided the highest accuracy.  In terms of GPU implementations, blah was the fastest while blah was the most accurate.  \gls{bp} \emph{is} amenable to parallelisation (unlike its traditional rival, Graph Cuts \cite{Tappen2003}) and GPU implementations, but the main point of interest for it in this work is the fact that it is explicitly built around the concept of independent processing elements exchanging messages.

\subsubsection{Real-time/resource-constrained \glsentrylong{bp}}
One of the major drawbacks of \gls{bp} as compared to a number of other approaches to \gls{sm} is that a simple na√Øve implementation is both quite slow, and very memory-intensive.  Slow because of the requirement to perform many iterations, and memory-intensive because \emph{at least} one copy of the data costs and the message estimates for each neighbour must be stored in memory, with the result that a number of values on the order of at least \(O(5XYD)\) are kept in memory, where X and Y are the width and height of the stereo images, and D is the size of the disparity range.

Seeking to derive the comparative benefits of a global stereo algorithm without compromising resource and time requirements too much, there have been a number of attempts at a real-time \gls{bp} algorithm \cite{Xiang2012,Yang2010,Yang2006,Liang2011,Gupta2012,Perez2010,Felzenszwalb2006}.

Felzenszwalb \& Huttenlocher \cite{Felzenszwalb2006} made three significant improvements:  i) They demonstrated a way to reduce the complexity of the message update process from \(O(|D|^2)\) to \(O(|D|)\) (where \(|D|\) is the total number of potential disparity labels).  ii) They showed that, because each pixel relies entirely upon the messages received from its neighbours at the previous iteration, only half of the pixels in fact need to be updated in a given iteration, without affecting the final results.  This both halves the number of message computations required at iteration, but moreover means that only a single copy of the messages need be kept while ensuring that messages computed earlier in an iteration have no impact upon messages computed later.  iii)  They introduced a hierarchical approach, where the first iterations were performed over a much smaller grid, representing an amalgamation of the actual grid, but later iterations would operate over larger and larger grids until reaching the full size.  This had the benefit of propagating information across the grid in a much faster fashion, with relatively little loss in accuracy.  Almost every claimed real-time \gls{bp} algorithm since uses the hierarchical approach.

Yang \textit{et al.} \cite{Yang2006} claimed that they had devised a new approach that would provide a 45x speedup, and boasted that their system could achieve a frame rate of 16 \gls{fps} on a 320 x 240 image with 16 disparity levels.  This claim was largely based, however, in the fact that they used a GPU to implement it -- and then later stated that they had not yet implemented their method on a GPU.  Furthermore, they did not present anything that had not already been described by Felzenszwalb \& Huttenlocher.

Yu \textit{et al.} \cite{Yu2007} presented a proposed approach for compressing the messages, thus reducing total memory occupied, but it has not proven popular.  This may be because it is not amenable to parallelisation, thus significantly reducing its practicality \cite{Yang2010}.

Yang, Wang \& Ahuja \cite{Yang2010} proposed an approach which they claim needs only constant memory space, regardless of the number of disparities involved, while still returning results that are almost as accurate.  For example, they claim that for an image with 800 x 600 pixels and 300 disparity levels, their algorithm requires only around \SI{9}{\mebi\byte} of memory -- it is not clear though whether they include storing the computed data costs in that amount or not.  The main element of their approach is that as they move from the coarser levels of the hierarchy, they proportionally reduce the number of disparity labels considered at each level, keeping the total memory required constant.  %This leads to an issue in that, should the true disparity not be selected for inclusion at a reduction, that pixel will never see the correct disparity label assigned to it.  To work around this, they 

Gupta \& Cho \cite{Gupta2012} used 3x3 tiles in their hierarchical method, rather than the usual 2x2.  This meant that their process was somewhat faster overall, and means that at the more coarse levels they need less memory.  The other main differences between their method and previous ones are that they use an `alternative schedule method' borrowed from \cite{Tappen2003}; and they use a different disparity refinement operation as final step.  The results, in terms of accuracy and speed, do not appear to be any better than earlier papers, though.

Xiang \textit{et al.} \cite{Xiang2012} also boasted of a new technique that enabled faster speeds, but again their implementation largely merely borrowed concepts from \cite{Felzenszwalb2006} and used a GPU.  They did improve accuracy results, however, by incorporating Yoon \& Kweon's \cite{Yoon2005} adaptive support-weight approach as a post-processing step, with minimal extra computational requirements.

Tan \textit{et al.} \cite{Tan2017} claim that their fully-connected \gls{bp} method is highly-amenable to parallelisation, suggesting it could be implemented to run in real-time, but they do not appear to have done so themselves.

% \subsection{Semi-global Matching}

\subsection{\glsentrylong{cp}}

\cite{Gong2015,Gong2013a}

\section{Formal Models of Concurrent Computation}
Perhaps the earliest (or at least, the earliest that is still widely known) model of concurrent computation is the Petri Net.

\cite{Varela2013}

\subsection{\glsentrylong{csp} \& Pi~Calculus}
\gls{csp} is a `process algebra' and abstract model of concurrent computation put forward by Hoare \cite{Hoare1985,Roscoe2011}.  A typical sequential computation is represented by a `process'.  Processes' ``behaviour is described in terms of the occurrence and availability of abstract entities called \textit{events}'' \cite[p.~478]{Roscoe2011}.  Should more than one event be available simultaneously for a given process, then one will be chosen non-deterministically.  This choice is internal to the process, and not influenced by or visible to any other process.  %E.g. for a situation where there is one possible event \(a\) at a given point in time, the process \(P\) will choose that event and then proceed according to the result, written as: \[ a \rightarrow P(a) \]

Concurrency is introduced by the existence of multiple processes.  In general, the processes evolve independently, responding to events as they come.  Should a particular event appear in the alphabet of multiple processes, however, then all processes \emph{must} choose to participate in that event at the same time.  Should all processes involved make such a choice, they engage in a synchronous multi-way atomic synchronisation (hence `communicating').  \gls{csp} has provided significant inspiration for concurrency design in a number of programming languages, including notably Ada [ref], Occam [ref], Google's Go [ref] and \gls{cml} \cite{Reppy2011}.

% Milner had earlier created the Calculus of Communicating Systems [ref] as a relatively early attempt to model concurrent computing.  

Milner appreciated \gls{csp}, which advanced concurrent models by explicitly incorporating \emph{synchronised} interaction, something Milner's earlier Calculus for Communicating Systems [ref] had lacked.  Milner still regarded \gls{csp} as incomplete, however, in that it had no support for the concept of `mobility' -- i.e. the ability of the system to reconfigure itself during operation.  Pi Calculus was created as an attempt to build upon those earlier systems but present a complete calculus of concurrent computation in much the same way that Lambda Calculus [ref] is a complete calculus for sequential computation \cite{Milner1993}.\footnote{Milner also pointed out that sequential computation is in fact a special case of concurrent computation.}

\subsection{\label{subsec:actors}Actors}
The Actor model was introduced by Hewitt \cite{Agha1986}.  Much like \gls{csp} \& its cousins, the Actor model is entirely based around the concept of separated, sequential but communicating processes which exchange messages.  Again, the processes make decisions and proceed based on their communications.  A key difference, however, is that in the Actor model the message exchanges are \emph{a}synchronous.  Each actor has its own `mailbox', and may send messages to other actors so long as it knows their name (which is equivalent for this purpose to a concept of an address for the actor), \emph{but does not wait at all for a response before proceeding}.

The Actor model is a popular one for concurrent programming, possibly owing to its intuitive concept.  The fact that communication is asynchronous makes Actors much more suitable for modelling distributed systems without shared memory than \gls{csp} or similar -- Actors can send messages and proceed without (necessarily) needing to wait for a response, instead continuing to process based on the messages they themselves have received.  By contrast, a system with synchronous communication would have prohibitive time costs, given the relative slow speed of typical links between distributed computers as compared to their capacity for local processing.  Many Actor systems have been implemented for different programming languages [refs], and in fact it is at the very core, and perhaps largely responsible for the success, of Erlang [ref].

\subsection{Join Calculus}


\subsection{Others}

Gorlatch \cite{Gorlatch2004} argued against basic message passing, decrying it as an unnecessary and unhelpful complication and favouring `collective' operations instead.  This criticism focused upon \gls{mpi} as it was at the time, however, and made no reference to either Actors or \gls{csp}.

\subsection{\glsentrytext{ps}/\glsentrytext{mc}}
\gls{ps}, also known as \gls{mc} (the two terms are generally used interchangeably), is a bio-inspired model of computing created by PƒÉun in the late 1990s \cite{tPaun98a,Paun2000}, originally conceived of by considering the process of chemical reactions and exchanges that occur inside living biological cells \& the membranes within, and regarding this process as a form of computation.

PƒÉun describes \gls{mc} \cite[p.~VII]{Paun2002} as:
\begin{quote}
Membrane computing is a branch of natural computing which abstracts from
the structure and the functioning of living cells. In the basic model, the membrane
systems - also called P systems - are distributed parallel computing
devices, processing multisets of objects, synchronously, in the compartments
delimited by a membrane structure. The objects, which correspond to chemicals
evolving in the compartments of a cell, can also pass through membranes.
The membranes form a hierarchical structure - they can be dissolved, divided,
created, and their permeability can be modified. A sequence of transitions between
configurations of a system forms a computation. The result of a halting
computation is the number of objects present at the end of the computation
in a specified membrane, called the output membrane. The objects can also
have a structure of their own that can be described by strings over a given
alphabet of basic molecules - then the result of a computation is a set of
strings.
\end{quote}

\Gls{mc} works analogously to a typical modern electronic computer, in that the system stores data, and processes \& updates those data based on a predefined program, with a view to arriving at a computable answer based on the starting state and any inputs to the system \cite{Paun2002,Paun2010b}.  In the case of \gls{ps}, the data are multisets of symbols, representing various chemicals and their quantities.  These are found inside one or more cells, based on real biological cells, which (to a certain extent at least) form a hybrid between main memory and the processing units of a computer.  The instructions of the program itself are provided by rules, which specify transformations of objects and interactions with the surrounding environment and other membranes or cells.

There are now, broadly, three main families of \gls{ps} variants:  \gls{clps}, \gls{tlps} \cite{tMaPaPaRo01a,Martin-Vide2003} and \gls{snps} \cite{Ionescu2006}.\footnote{Other variants have been created, but most are used infrequently, if ever.  Apart from \gls{cps}, described in \autoref{subsec:cpsys}, this work will not address them.}  \gls{clps} is the original, and sees objects compartmented into `membranes', which are arranged in a graphical tree structure with the outermost membrane (separating the cell from its environment) as the root of the tree.  In most variants, objects can evolve inside a membrane, but also be communicated between membranes (and the environment).  Furthermore, membranes can divide or dissolve themselves, and may have one or more special properties, such as `polarization'.

Conversely, \gls{tlps} and \gls{snps} both arrange their computing compartments, named `cells' or `neurons' respectively, as nodes in arbitrary digraphs, with the edges between them representing connecting channels.  Whereas \gls{clps} emphasise the evolution of multisets of objects inside compartments, \gls{tlps} and \gls{snps} emphasise communication between separate cells/neurons, and many \gls{tlps} variants do not include any capacity for internal evolution inside cells -- if new objects are required, they are imported via communication with the environment, which is considered to possess an unlimited number of all objects, but has no rules of its own.

While \gls{tlps} have arbitrary alphabets, only one object is used in \gls{snps}, the `spike'.  This means that \gls{tlps} are frequently much like \gls{clps} ones in that they have custom objects for each purpose, with the key difference (usually) being in how the \glspl{prox} are arranged relative to each other and the choice between the two motivated primarily by which one seems like a better fit to the computation to be modelled.  Conversely, \gls{snps} force everything to be represented through the use of differing quantities of the spike, kept in different neurons.  This means that it can be more complex to model certain problems, but also arguably means that \gls{snps} are, \textit{prima facie}, closer to Lambda Calculus [ref] and Church Numerals [ref], as well as Register Machines[ref] (and indeed Register Machines have been simulated with \gls{snps}[ref]).  All three approaches have been proven Turing-universal though[ref], so all three should be capable of expressing the same computations in different forms.

Arguably, the most notable and important aspects of \gls{ps} models are that they:  i) Generally have no space limit.  That is, they contain an arbitrary number of cells, objects and membranes;  ii) Across all cells and membranes, all rules that can be applied are applied, as many times as possible given the current number of objects available.  These two features mean that \gls{ps} have unbounded space and processing capacity, which can be used to solve traditionally computationally-difficult problems relatively quickly \cite{Sosik2003,Jimenez2003,Paun1999a}.  Most of these solutions, however, rely on trading time complexity for space complexity.  While this works in the theoretical framework, electronic simulations of the systems do not have access to unlimited instantaneous memory space, meaning many of the fast solutions are impractical with current real-world computers [refs].

\Gls{mc} is not just a theoretical model, with limited practical use.  Besides Image Processing \& Computer Vision (see \autoref{subsec:imgprocpsys}), \gls{ps} variants have been applied to a range of fields, from power grid management to robotic control systems \cite{Zhang2017}.  [P-Lingua and simulation systems, e.g. MeCoSim]

\subsubsection{\label{subsec:cpsys}\glsentrylong{cps}}
\cite{Nicolescu2014b,Nicolescu2017}

\gls{cps} is another variant of \gls{ps}, developed by Nicolescu and collaborators in the early 2010s [ref].  It is largely based on \gls{clps}, and can be seen, to some extent at least, as a higher-level abstraction over it \cite{Nicolescu2018}.  It can also incorporate elements of \gls{tlps}, however, in that it includes concepts of channels and message passing between cells \cite{Henderson2019}.  Nicolescu, Ipate \& Wu demonstrated that not only is \gls{cps} capable of performing the same tasks as other \gls{ps} variants, but also can be used fairly cleanly to model typical computer programs \cite{Nicolescu2014a}.

The major advantages of \gls{cps} over traditional \gls{clps} is a simplification in the specification of complete systems to solve a given problem.  \gls{clps} (as well as \gls{tlps} and \gls{snps}) typically require the definition of a family of rulesets customised to the specific instance of the problem at hand, whereas \gls{cps} usually requires only the definition of a fixed (usually much shorter) set of rules that cover all possible instances.  Only inputs to the system need vary to solve different instances of the problem, e.g. in \cite{Cooper2019} only five fixed rules were needed to solve any instance of the Travelling Salesman Problem, with only customisation of the input objects (in this case, elements describing the nodes and edges of the graph) required.



\subsubsection{\label{subsec:imgprocpsys}Image Processing and Computer Vision in P~systems}
\cite{Zhang2012}

Perhaps owing to the unbounded potential space and parallelism of \gls{ps}, combined with the embarrassingly parallel nature of many tasks in Image Processing \& Computer Vision, the latter has proved to be fertile ground for the former, although not every publication puts its model to the test with a computerised simulation, or if it does, the authors may only provide scant details \cite{Diaz-Pernil2019}.  

Christinal, D√≠az-Pernil \& Real \cite{Christinal2011} described a family of \gls{tlps} to perform region-based segmentation of both 2D and 3D images.  Despite their family of systems requiring only two cells, it also needed custom rule sets based on the size of the images as well as the number of colours present, with a number of rules per set proportional to the same measurements.  The paper showed the results of simulating the system, but provides no details on performance.

D√≠az-Pernil \textit{et al.} \cite{Diaz-Pernil2013} commented that ``... commonly [a] parallel algorithm needs to be re-designed with only slight references to the [sequential original].  ... the design of a new parallel implementation not inspired by the sequential one allows ... the proposal of new creative solutions.''  They then demonstrated this fact by designing a new edge detection and segmentation algorithm named `A Graphical P (AGP) segmentator', inspired by the Sobel operator [ref] and using the segmentation method from \cite{Christinal2011}, which they modelled in \gls{tlps}.  The authors implemented their new algorithm on a \gls{gpu} and compared it with an implementation of the 3x3 and 5x5 Sobel operators, finding that theirs had near-identical runtimes but superior edge detection capabilities.

D√≠az-Pernil \textit{et al.} \cite{Diaz-Pernil2013a} further explored modelling classic image processing techniques by implementing Guo \& Hall's binary image skeletonisation technique \cite{Guo1989} with \gls{snps}.  The overall system's rules templates are reasonably simple, but include references to a set \(DEL\) (used as a lookup to determine whether a cell should turn white or stay black) which does not appear to be modelled inside the system, meaning that it is not self-contained.  The authors simulated this system on a \gls{gpu}, but found that their implementation was upwards of twice as slow as another pre-existing implementation.  Confusingly, however, they state that one of the reasons for this is ``that the use of an alphabet with only one object, the spike \(a\), does not fit in the GPU architecture''.  This statement is difficult to understand, given that spikes can easily be represented as simple integers.  The authors also commented that the synchronous nature of the model is unrealistic, and imposing a global clock upon the system can be problematic.

Nicolescu \cite{Nicolescu2014} alternatively applied \gls{cps} to image skeletonisation based on Guo \& Hall's technique \cite{Guo1989}, presenting three forms of a solution: Synchronous versions that use multiple or a single cell (essentially the latter replicates the former via the use of sub-membranes), and an asynchronous multi-cell version.  The asynchronous version no longer assumes that all messages are passed between cells simultaneously and instantaneously, compensating for this by increasing the number of messages used.  This form, while arguably more realstic to modern computers, requires a greater message complexity. A prospective Actor-model-based (see \autoref{subsec:actors}) simplified implementation using \fsharp{}'s \texttt{MailboxProcessor} \cite[ch.~11]{Syme2015a} was presented also, but no results from running it were reported.

Nicolescu \cite{Nicolescu2015a} further applied \gls{cps} to seeded region growing of grayscale images.  The described system used a two-level approach, based on the `Structured Grid Dwarf' of the 13 Berkeley Dwarves \cite{Asanovic2006}, where the image was divided into rectangular blocks of multiple pixels.  Each block was modelled with a single cell, inter-block processing was carried out via message passing, and intra-block processing was performed by typical object evolution.  It was again suggested that this would fit well to the Actor model.

D√≠az-Pernil \textit{et al.} \cite{Diaz-Pernil2016} built upon their AGP segmentator algorithm to create a version that works with RGB images rather than grayscale and applied it to a common medical Computer Vision task, isolating the `optic disc' in images of the inner eye.  With this they used the skeletonisation algorithm from \cite{Diaz-Pernil2013a} and a number of other steps not based on \gls{ps} to produce a complete imaging pipeline.  The authors implemented this on a \gls{gpu}, and found that their system was both more accurate and faster than previous systems.

Most directly relevant to the current work are \cite{GimelFarb2013a,Gimelfarb2011,Nicolescu2014b}, which model Dynamic Programming \gls{sm} in \gls{ps}, and indeed saw the genesis of \gls{cp}.  

\subsubsection{P~systems on \glsentrylongpl{gpu}}
In many instances, a \gls{ps} model for a problem involves many separate small elements processing their data separately, and perhaps updating each other's state at the end of a step.  Given that this sounds remarkably close to the Single-Instruction Multiple-Thread[ref] nature of modern \gls{gpgpu}, it is no surprise that there has been much work put into simulating \gls{ps} on \glspl{gpu}.

\cite{Cecilia2010,Cecilia2010a,Cecilia2013,Macias-Ramos2015,Martinez-Del-Amor2015,Martinez-Del-Amor2013a,Maroosi2014,Maroosi2014a}

\section{\glsentrytext{cml} \& related}

\Gls{cml} [ref] is an approach to concurrent programming originally developed by Reppy (based on his earlier `Pegasus Meta-Language' \cite{Reppy1988}).  It was created originally as a library in Standard ML of New Jersey, but its concepts have subsequently appeared elsewhere.   \Gls{cml} is designed to avoid many of the problems with concurrency that arise in traditional sequential programming, where the use of locks, mutexes and semaphores etc. are frequently required, and often lead to the potential introduction of problems such live/deadlocks, data races and extreme resource contention.  This is achieved by changing the approach to concurrent programming to one of logically separate, internally-sequential, processing elements that share data as required by `passing messages'\footnote{This is the logical concept, but there is not strictly any specific required software implementation.} between themselves.  In CML, these logical processing elements are referred to as threads, and they exchange messages over channels synchronously, i.e. there is a temporal overlap between one thread offering to send, and another to receive, over the same channel, and the first to offer blocks until the second makes its offer.  Hereafter in this paper, unqualified use of the word `thread' refers to CML's threads, as opposed to other meanings of the term.

Reppy describes a concurrent program as one that supports multiple sequential sub-programs conceptually executing in parallel separately, but interacting through shared resources to achieve a common goal.  CML is concerned with the scenario where said interactions are explicit, and in order to facilitate that ``CML takes the unique approach of supporting \emph{higher-order concurrent programming},'' (emphasis Reppy's) whereby communication and synchronisation are made into first-class members of the language, in a similar way to how functional programming languages made functions into first-class members of themselves \cite[Preface]{Reppy2007}.

% \section{Measures of `code quality'}
% Measuring code quality is a small subset of a much larger topic, often called, amongst other names, Software Quality Assurance or Software Quality Engineering.  The broader field is largely concerned with ensuring that software meets quality requirements, where quality may be defined in a multitude of ways.  Much of the field is concerned with managing processes for professional developers and organisations which employ them, as opposed to 

% \subsection{Why should one care about code quality in this context?}
% Stereo matching algorithms\footnote{For the purposes of this subsection specifically, unless stated otherwise, `algorithm' shall be used in the sense of referring to particular implementations of the theoretical algorithms} tend to be relatively self-contained -- one can simply treat them as black boxes whereby a pair (or more) of rectified images are passed in, and a disparity map returned.  One might wonder why exactly there is any reason to consider concepts of code quality in this context.

% A number of reasons to consider it are listed below:
% \begin{itemize}
% \item \textbf{Maintainability} -- Inevitably, changes in the environment a particular algorithm is to run in will occur over time.  Eventually, with enough of these changes accumulated, the algorithm will need to be updated in order to ensure that it still continues to run and produce accurate results.  Better quality code can make such changes significantly easier.
% \item \textbf{Changes in maintainers} -- In almost all circumstances outside of academic research, algorithms are not written once and then never looked at again.  Instead, they will be revised at some point as part of necessary maintenance.  It is highly likely that, sooner or later, the person(s) tasked with maintaining the algorithm will change.  While it is plausible that both the old and new maintainers might understand the theoretical underpinnings of an algorithm (though that is by no means guaranteed in general), one inevitably needs time to learn how to work within a new codebase.  Better quality code will make the transition between maintainers significantly easier, and likely lead to the new maintainer becoming productive quicker.
% \item \textbf{Ease of experimentation} -- better code (especially more well factored out code) means that one can probably experiment with different approaches more easily.
% \item \textbf{Upgradability} -- Related to maintainability,\footnote{Both maintainability and upgradability are closely linked to the concept of `technical debt', which is [insert definition here].} it is likely that with new developments in hardware, operating systems, or stereo matching theory, new, more efficient, ways of implementing algorithms may become possible.  An uncoordinated mess of spaghetti code could prove extremely difficult to modify appropriately, with the result that attempts to upgrade the operation of an algorithm may fail, or at least not achieve the success aimed for.  Better quality code, however, will in most cases make such upgrades both quicker to perform, and likely to succeed.
% \end{itemize}

% \subsection{Dimensions of quality}
% See that standard ISO/IEC 25010:2011.  

% In this work, the focus will be entirely upon metrics that can be computed via static analysis.  The are multiple reasons for that.  For one, much of the measurement of software quality is performed over a relatively long timescale, whereby measures such as the number of bugs reported, customer satisfaction, or some other concept of how fit for purpose the final product was, are used.  Secondly, the measurements are typically taken at a much larger scale than that of individual algorithms, which in this case are loosely analogous to individual function calls.  Thirdly, a considerable number the metrics are qualitative in nature, requiring judgement calls on the part of a reviewer to determine the quality of the code.

% Limiting the focus in this work purely to static measures will provide quantitative data that are as close to objective and comparable as can reasonably be achieved, and which can be calculated purely on the basis of the available source code, without needing to run it at all.  Note that even using static metrics may not provide full objectivity, as there will inevitably differences across programming languages that can be taken into account and controlled for entirely.

% \subsection{Metrics chosen}
% \begin{itemize}
% \item SLoC
% \item Cyclomatic complexity
% \item Ease of reading measures?
% \item Nested expressions?
% \item From Alec et al.'s paper on the Santa Claus problem - ratio of uncompressed to compressed code files - the less it can be compressed, the more expressive the current code would appear to be. "The raw/compressed ratio is intuitively a measure of the expressiveness: the lower the ratio, the better: less noise/redundancy and higher information density."
% \item What else?
% \end{itemize}
