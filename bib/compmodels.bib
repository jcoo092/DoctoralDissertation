@book{Varela2013,
abstract = {An introduction to fundamental theories of concurrent computation and associated programming languages for developing distributed and mobile computing systems. Starting from the premise that understanding the foundations of concurrent programming is key to developing distributed computing systems, this book first presents the fundamental theories of concurrent computing and then introduces the programming languages that help develop distributed computing systems at a high level of abstraction. The major theories of concurrent computation—including the $\pi$-calculus, the actor model, the join calculus, and mobile ambients—are explained with a focus on how they help design and reason about distributed and mobile computing systems. The book then presents programming languages that follow the theoretical models already described, including Pict, SALSA, and JoCaml. The parallel structure of the chapters in both part one (theory) and part two (practice) enable the reader not only to compare the different theories but also to see clearly how a programming language supports a theoretical model. The book is unique in bridging the gap between the theory and the practice of programming distributed computing systems. It can be used as a textbook for graduate and advanced undergraduate students in computer science or as a reference for researchers in the area of programming technology for distributed computing. By presenting theory first, the book allows readers to focus on the essential components of concurrency, distribution, and mobility without getting bogged down in syntactic details of specific programming languages. Once the theory is understood, the practical part of implementing a system in an actual programming language becomes much easier.},
address = {Cambridge, Mass.},
author = {Varela, Carlos A},
isbn = {9780262018982},
pages = {296},
publisher = {MIT Press},
title = {{Programming Distributed Computing Systems : A Foundational Approach}},
url = {https://mitpress.mit.edu/books/programming-distributed-computing-systems},
year = {2013}
}

@techreport{Asanovic2006,
abstract = {The recent switch to parallel microprocessors is a milestone in the history of computing. Industry has laid out a roadmap for multicore designs that preserves the programming paradigm of the past via binary compatibility and cache coherence. Conventional wisdom is now to double the number of cores on a chip with each silicon generation. A multidisciplinary group of Berkeley researchers met nearly two years to discuss this change. Our view is that this evolutionary approach to parallel hardware and software may work from 2 or 8 processor systems, but is likely to face diminishing returns as 16 and 32 processor systems are realized, just as returns fell with greater instruction-level parallelism. We believe that much can be learned by examining the success of parallelism at the extremes of the computing spectrum, namely embedded computing and high performance computing. This led us to frame the parallel landscape with seven questions, and to recommend the following: The overarching goal should be to make it easy to write programs that execute efficiently on highly parallel computing systems The target should be 1000s of cores per chip, as these chips are built from processing elements that are the most efficient in MIPS (Million Instructions per Second) per watt, MIPS per area of silicon, and MIPS per development dollar. Instead of traditional benchmarks, use 13 "Dwarfs" to design and evaluate parallel programming models and architectures. (A dwarf is an algorithmic method that captures a pattern of computation and communication.) "Autotuners" should play a larger role than conventional compilers in translating parallel programs. To maximize programmer productivity, future programming models must be more human-centric than the conventional focus on hardware or applications. To be successful, programming models should be independent of the number of processors. To maximize application efficiency, programming models should support a wide range of data types and successful models of parallelism: task-level parallelism, word-level parallelism, and bit-level parallelism. Architects should not include features that significantly affect performance or energy if programmers cannot accurately measure their impact via performance counters and energy counters. Traditional operating systems will be deconstructed and operating system functionality will be orchestrated using libraries and virtual machines. To explore the design space rapidly, use system emulators based on Field Programmable Gate Arrays (FPGAs) that are highly scalable and low cost. Since real world applications are naturally parallel and hardware is naturally parallel, what we need is a programming model, system software, and a supporting architecture that are naturally parallel. Researchers have the rare opportunity to re-invent these cornerstones of computing, provided they simplify the efficient programming of highly parallel systems.},
address = {Berkeley, CA},
author = {Asanovic, Krste and Bodik, Rastislav and Catanzaro, Bryan Christopher and Gebis, Joseph James and Husbands, Parry and Keutzer, Kurt and Patterson, David A. and Plishker, William Lester and Shalf, John and Williams, Samuel Webb and Yelick, Katherine},
file = {:C$\backslash$:/Users/jcoo092/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Asanovic et al. - 2006 - A view of the parallel computing landscape.pdf:pdf},
institution = {University of California at Berkeley},
month = {12},
pages = {56},
% publisher = {EECS Department, University of California, Berkeley},
title = {{A view of the parallel computing landscape}},
url = {http://www.eecs.berkeley.edu/Pubs/TechRpts/2006/EECS-2006-183.html},
year = {2006}
}
@article{Gorlatch2004,
abstract = {During the software crisis of the 1960s, Dijkstra's famous thesis "goto considered harmful" paved the way for structured programming. This short communication suggests that many current difficulties of parallel programming based on message passing are caused by poorly structured communication, which is a consequence of using low-level send-receive primitives. We argue that, like goto in sequential programs, send-receive should be avoided as far as possible and replaced by collective operations in the setting of message passing. We dispute some widely held opinions about the apparent superiority of pairwise communication over collective communication and present substantial theoretical and empirical evidence to the contrary in the context of MPI (Message Passing Interface).},
author = {Gorlatch, Sergei},
doi = {10.1145/963778.963780},
file = {:D$\backslash$:/jcoo092/OneDrive - The University of Auckland/Readings/Lit Review papers/Concurrent ML/Gorlatch - 2004 - Send-Receive Considered Harmful Myths and Realities of Message Passing.pdf:pdf},
issn = {01640925},
journal = {ACM Transactions on Programming Languages and Systems},
keywords = {Message Passing Interface (MPI),Programming methodology},
month = {1},
number = {1},
pages = {47--56},
title = {{Send-Receive Considered Harmful: Myths and Realities of Message Passing}},
url = {http://portal.acm.org/citation.cfm?doid=963778.963780},
volume = {26},
year = {2004}
}
@incollection{Roscoe2011,
address = {Boston, MA},
author = {Roscoe, A. W. and Davies, Jim},
booktitle = {Encyclopedia of Parallel Computing},
doi = {10.1007/978-0-387-09766-4_2073},
pages = {341--341},
publisher = {Springer US},
title = {{Communicating Sequential Processes (CSP)}},
url = {http://link.springer.com/10.1007/978-0-387-09766-4{\_}2073},
editor = {Padua, David},
year = {2011}
}
@book{Hoare1985,
abstract = {This book introduces a new mathematical approach to the study of concurrency and communication. Most suitable application of this new field is to the specification, design and implementation of computer systems which continuously act and interact with their environment.},
address = {Englewood Cliffs, N.J.},
author = {Hoare, Charles Antony Richard},
isbn = {0131532898},
keywords = {Communicating Sequential Processes,Computer programming,Parallel processing (Electronic computers)},
pages = {256},
publisher = {Prentice/Hall International},
series = {Prentice-Hall international series in computer science},
title = {{Communicating sequential processes}},
year = {1985}
}
@article{Milner1993,
author = {Milner, Robin},
doi = {10.1145/151233.151240},
file = {:D$\backslash$:/jarak/Documents/Milner - 1993 - Elements of Interaction Turing Award Lecture.pdf:pdf},
issn = {15577317},
journal = {Communications of the ACM},
keywords = {CCS,Communicating Sequential Processes,Pi Calculus,interaction,naming and reference,pi calculus,process algebra,process calculus,reduction rule},
month = {1},
number = {1},
pages = {78--89},
title = {{Elements of Interaction: Turing Award Lecture}},
url = {http://portal.acm.org/citation.cfm?doid=151233.151240},
volume = {36},
year = {1993}
}
@book{Agha1986,
address = {Cambridge, Mass.},
author = {Agha, Gul A},
isbn = {0262010925},
keywords = {Actors,Electronic data processing -- Distributed processi,Parallel processing (Electronic computers)},
publisher = {MIT Press},
series = {MIT Press series in artificial intelligence},
title = {{ACTORS : a model of concurrent computation in distributed systems}},
year = {1986}
}
@book{Padua2011,
abstract = {Containing over 300 entries in an A-Z format, the Encyclopedia of Parallel Computing provides easy, intuitive access to relevant information for professionals and researchers seeking access to any aspect within the broad field of parallel computing. Topics for this comprehensive reference were selected, written, and peer-reviewed by an international pool of distinguished researchers in the field. The Encyclopedia is broad in scope, covering machine organization, programming languages, algorithms, and applications. Within each area, concepts, designs, and specific implementations are presented. The highly-structured essays in this work comprise synonyms, a definition and discussion of the topic, bibliographies, and links to related literature. Extensive cross-references to other entries within the Encyclopedia support efficient, user-friendly searchers for immediate access to useful information. Key concepts presented in the Encyclopedia of Parallel Computing include; laws and metrics; specific numerical and non-numerical algorithms; asynchronous algorithms; libraries of subroutines; benchmark suites; applications; sequential consistency and cache coherency; machine classes such as clusters, shared-memory multiprocessors, special-purpose machines and dataflow machines; specific machines such as Cray supercomputers, IBM's cell processor and Intel's multicore machines; race detection and auto parallelization; parallel programming languages, synchronization primitives, collective operations, message passing libraries, checkpointing, and operating systems. Topics covered: Speedup, Efficiency, Isoefficiency, Redundancy, Amdahls law, Computer Architecture Concepts, Parallel Machine Designs, Benmarks, Parallel Programming concepts {\&} design, Algorithms, Parallel applications.{\&}160;This authoritative reference will be published in two formats: print and online. The online edition features hyperlinks to cross-references and to additional significant research. Related Subjects: supercomputing, high-performance computing, distributed computing},
address = {Boston, MA},
booktitle = {Encyclopedia of Parallel Computing},
doi = {10.1007/978-0-387-09766-4},
editor = {Padua, David},
isbn = {978-0-387-09765-7},
publisher = {Springer US},
title = {{Encyclopedia of Parallel Computing}},
url = {http://link.springer.com/10.1007/978-0-387-09766-4},
year = {2011}
}
